{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome!","text":""},{"location":"#welcome","title":"Welcome!","text":"<p>\u6b22\u8fce\u6765\u5230Kevin\u7684\u4e2a\u4eba\u4e3b\u9875\uff01\u8fd9\u91cc\u5c06\u4f1a\u6536\u5f55\u4e00\u4e9b\u8bfe\u7a0b\u7684\u5b66\u4e60\u7b14\u8bb0\uff0c\u5404\u7c7b\u8d44\u6e90\uff0c\u6709\u8da3\u7684\u4e1c\u897f\uff0c\u4ee5\u53ca\u4e00\u4e9b\u60f3\u6cd5\u3002 Notes Tools</p>"},{"location":"Casual%20notes/2024-10-17/","title":"2024 10 17","text":""},{"location":"Casual%20notes/2024-10-17/#recently-briefed","title":"Recently Briefed","text":"<p>Having been in Singapore for over 2 months. I have already got used to life here and found a comfortable living pace to follow. Buying durian at Fairprice, jurong points roughly twice a week. (It's not a season for MSW, red prawn and many other super delicious durian varieties grown in Malaysia. It is believed that many varieties sold these days were imported from Thailand. But still much betther than what I've taste in Pattaya back in recess week half month ago), found the best mix rice and vegie food stall near hall 11, do some indoor exercise on a daily basis. Cutting all the studpid lectures and spend half hour to make up for a 2-hour lecture. Staying up late till 2 or 3 reading Lord of the Mysteries and get up at later morning. Going to iot lab after lunch and leave at 5:30 for a lunch. As for the hours left are all mine, I can do whatever I want. Play MHR with BC, play street fighter 6, spend an hour to do some push-ups, have a bowl of instant noodles, scrolling bilibili, revise lecture slide making some preparation for upcoming labs. Everyday can be a flawless work-life balance. </p>"},{"location":"Casual%20notes/2024-10-17/#whats-coming","title":"What's coming","text":"<p>Gamescom Asia will take place in Singapore from 18-20 october. During which, the SF6 super premiere will be held. Xiaohai, Gachikun, Tokido along with many other top players will be there. Really looking forward to it for it's my first time to attend a SF6 offline event as an audience. Hope I got the chance to watch the grand final and collect some selfies or signatures from the top players!</p>"},{"location":"Casual%20notes/2024-10-17/#some-insights-and-thoughts","title":"Some insights and thoughts","text":"<ul> <li>I tried to listen to some podcasts lately. The first I tried is Deep Dive with Ali Abdaal. He upload podcast videos on Youtube every wee. Invite some guests to share some insight while having a genuine conversation. Though he's pronunciation is clear enough, I still struggle to catch up with the content. Especially when they are chatting more casually. Filler words, slangs would come out frequently. I mean, when they are trying to express the idea which just occured to him, the whole sentence might not be so organized. It might have a strange structure. Plus my identity of a non-native speaker, the logic of the sentence in Chiness is already different from that in English, making it even harder to understand. OFC I know I should cultivate a English thinking pattern, but it's never easy. And podcast, compared to TED talk, is more casual, more like a daily conversation. Honestyly, the only problem I found when listening to TED talk is vocabulary. Even if I don't know the exact meaning of a word, I can make a guess based on the context. But for podcast, it's more about the logic of the sentence, the structure of the conversation. </li> <li>Should or should not speak to natives at my level? I mean, I can understand what they are saying, but I can't express myself as fluently as they do. I remember one English related content creator on bilibili said that it's better you just listen to native speakers talks to each others. Cause if you rush to have a conversation as a beginer, it not only can be demoralizing, but also chances are that they have to lower their speaking speed and use simpler words to make you understand which makes the conversation less natural and limit the effectiveness of the practice. I totally agree with this idea. Judging from my experience with one native speaker who share the same course SC2106 with me. I helped him on the lab and we did have some chance to speak to each other. But I can definitely tell that the way he talks with me differs from the way he talks with his friends. So as a result I ddin't really improve to much from the conversation except some buzzwords and Singlish accent-wise.</li> <li>I do understand that exchange is a great opportunity to immerse myself in English and make some friends. Well, it turns out that in Singapore, you can just live with Chinese as comfortably as you do in China. So I have to force myself immerse in English by switch social media like Zhihu, bilibili to Twitter, Reddit and Youtube. And I have downloaded English version of Lord of The Mysteries which was originally written in Chinese. I believe I have made some progress in terms of scanning the content. But still, it's not as comfortable as Chinese to get information from text. For one thing, reading English text cost a lot of my attention and energy, making me dizzy after reading english content for over 30 minutes. On the other, there still a lot of words, references, memes that keep me from totally understanding the content. Failed to understand most of the post on Twitter without making a search or ask LLM is really frustrating... I desperately want to boost my English level to a point where I can understand the content without much effort. But god, the more I practice, the more exhausted I feel, the weaker I feel about my English level. I feel like I have reach a bottle neck. How to break through this? Is there anything wrong with my methdodology? Or is it just a matter of time in belief that quantity will eventually lead to quality? I have no clue.</li> </ul> <p>Anyway, that's all for today's casual notes. Tomorrow would be lab 4 of SC2106. Gonna make some preparation for it.</p> <p>12:00 AM, 17th October, 2024</p>"},{"location":"Casual%20notes/2024-11-03/","title":"2024 11 03","text":""},{"location":"Casual%20notes/2024-11-03/#recently-briefed","title":"Recently Briefed","text":"<p>It's been a while since last time I wrote a casual note. Just feel a bt lazy and tired lately, not feeling up to dwon for a new one. But well, I'm here again.  My research project hasn\u2019t been progressing smoothly lately, and I feel stuck. We wate two weeks of time trying to integarte the caric mission into aerial gym simulator(a isaacgym extension developed by NTNU) Cause we only found that isaac gym does not support a true multi-agent task and is deprecated by navida team already. So we have to switch to isaac lab and isaac sim which are now under maintenance and frequently updated. The good news is that it has great performance, make the most of our devices, also the tutorial provided by nvidia is great. However, it does not make the steep learning curve any easier. We have to learn the new API, the new environment, the new way to interact with the environment, the new way to train the model. It's like we have to start from scratch. We haven't even tried to integrate it with caric, we are already facing a bunch of problem. Phew, plus my exchange semester is coming to an end, I need to spare some time to preparing my final exam as well. I'm feeling sort of stressed out..</p>"},{"location":"Casual%20notes/2024-11-03/#about-th-super-premier-singapore","title":"About th Super Premier Singapore","text":"<p>Ah, let's write something about the super premier singapore as well. It's my first time to ever attend an offline fighting event. And it turned out to be a super great experience. TBH, it's completely different from watch a competition online. The vibe, the energy, the atomosphere, the excitement, the tension really raise your adrenaline. It's just so different. I was so lucky to sit on the second row right behind the players, like DCQ, Zhen, Oil king, Leshar... I really want to take as much selfie with all these top players as I can, but I don't want to bother them since they got games to play. Especially when it close to his turn or after he lost the game. And, you know, I have a little bit socialphobia. So at the end, I only take selfie with Tokido, Moke, Tachikawa and Endingwalker. But still\uff0c what a exprience. Perhaps one day I'll be on that stage as a player too. Who knows?</p>"},{"location":"Casual%20notes/2024-11-03/#games-played-recently","title":"Games played recently","text":"<ul> <li> <p>Monster Hunter Rise: I've just picked it up again with ZBC. Monster hunter wilds is coming on February 2025, there're still 3 month's vacancy, so he just want to play some MHR to kill this time. Although I've played MHR for nearly 300 hours, I didn't dive as deep as I did in MHWI which I've played for more than 1000hr and even once made some records. This time, after 2 or 3 weeks, we have reached M4, have to admit that, the design on the moveset and weapon machanics are exceptional. Having a much better experience than MHWI.</p> </li> <li> <p>Street Fighter 6: Ah, I'm getting a little bit tired to just stick to playing sim. And as I climb the rank, I realized that lacking of the knowledge of other characters really blocked my way to a higher rank. So, in the past few weeks, I dropped sim and tried to get some character to master rank as well. So far, I have Rashid, Honda, Dhalsim, Manon and DJ to master rank already. I'm working on Kimberly right now. It's tougher than I've expected. Since a lot people argue that once you have one character to the master rank, it would be much easier to get the rest to the same place. But even though I reached MR1700 with Dhalsim, it still took me one week or so to get one character to master. This game highly relies on muscle memory. So when I pick up a brand new character, besides the frame data, combo, frame kill, oki setup to learn, I have to spend some time forgetting the muscle memory of the previous character as well as forming the muscle memory of the new character. It's a tough job. Can't say I enjoy it, especially when you are making two much mistakes due to this muscle memory, whether dropping a combo, missing a punish, or even worse, doing a wrong move. But it feels great when I finally make one to the master rank.</p> </li> <li> <p>Monster Hunter Wilds Beta V0: Highly anticipated and eagerly awaited Wild-Beta has finally arrived. </p> </li> </ul>"},{"location":"Casual%20notes/2024-11-10/","title":"2024 11 10","text":""},{"location":"Casual%20notes/2024-11-10/#how-is-it-going","title":"How is it going?","text":"<p>It's less than a month before the end of my exchange semester. Looking back, I just feel a little bit down in the dumps. It's not quiet rich and full a semester. I felt sort of guilty of skipping all the lectures, staying up late every night and getting up when it's almost noon or even later. I didn't make any friends with locals or even have a deep conversation with other Chinese exchange students. Got stuck in a really bad routine and waste all my energy and time. As for the lab, it's no different from working, I could have make some pregress on the caric mission project but I failed so. Plus, I don't think I've boosted my English level as much as I expected. It's like a vicious circle: with my poor english level, I don't have the confidence to start a conversation with locals, thus letting the chance to improve my English slip away. Every time I scrolling through Twitter, seeing all those posted by native speaker, I be like, WTF are they talking about? Are we even using the same language? It's not only about the vacabulary, but also the logic and structure of a sentences and meme, cultural gap within. It makes me doubt my english proficiency and feel like I'm not making any progress. I even feel like I'm not gonna be assimilated into US or Singapore if I ever immigrate to these countries.  Ahh, so much negaticity. Sometimes I wonder if I have pushed myself too hard. Would life be easier if I just keep my desire minimal. Find a job in China, or even in my hometown, make an average level of income, perhaps never got married, never have kids, visit my parents once a week, play videos. Engaged in a career that I never feel passionate about, but at least I can make a living. I don't need to chase big, dream big, think big. Crammed on the subway with all the other workers, just going through the motions. At work, all I'm thinking about is what to eat after, what games to play or what shows to watch at night. In a crowd, you'll never tell me apart from anyone else, just another regular person, blending right in with the sea of salarymen. Maybe I can treat myself to a nice meal once a while, but in that way, It'll be hard to afford a house, a world tour. Even if I bought a house, I'll spent rest of my life paying off the mortgage. Isn't that a slave or a master of life? What if working for a huge Internet complex, with a high salary, but works from nine in the morning to nine at night, six days a week. Achiveing financial freedom at the age of 35. But still another screw in the huge machine of the company. I don't know. I'm just feeling a little bit lost momentarily.</p>"},{"location":"Casual%20notes/2024-11-10/#what-immigration-can-bring-to-me","title":"What immigration can bring to me?","text":"<p>With Trump winning the 2024 US election and poised to return to the White House next January. It is anticipated that the US will be less friendly to foreign immigrants. Plus the tension between China and the US, as well as the policies Trump has enacted during has last tenure, both J1 and H1B visas are likely to be more difficult to obtain. On top of that, robotics and AI are STEM related displines... The reformations of the US goverment with its new policies may have a direct impact on my summer research program, my choices and my future career. Worse still, thousands of Chinese students who are just as ambitious as me, who are trying to avoid the overwhelming and violent competition in China, who are seeking an oppotunity in the western world like US may have to change their destination to, for example Singapore or HK making the competition in these regions even more fierce. (It's already worse enough compared to western countries since there are so many Chinese students here. NTU and NUS are taking advantages of their high ranking in the world to attract Chinese students to pay the high tuition fee. However, local policies are not friendly for foreigner to find a decent job, only cheap labour are welcome. So you have to compete with other fellow Chinese master students.) I was so determined to immigrate to the US or Singapore, but now my stance is shaken. The culture gap seems to be unbridgeable. Chances are that my circle of friends will all be Chinese even if I move to another country. Plus, my parents are not very supportive on this issure, they insist I work in China wherever I finished my degree. I love them, and I have no siblings with me, so I have to take their old-age well-being into consideration as well. </p>"},{"location":"Casual%20notes/2024-11-10/#about-the-relations-of-china-and-the-us-john-mearsheimer","title":"About the relations of China and the US -- John Mearsheimer","text":"<p>OK, here is the thing. John Mearsheimer took a visit to China lately referred by the translator of his famous book The Tragedy of Great Power Politics. I happened to watch an exclusive interview of him and get to know some of his idea and his stance. I don't know him nor his point of view before, but I found it interesting and worth writing and re-thinking.</p> John Mearsheimer <p>Here's a little background of him. John Mearsheimer is a political scientist and international relations scholar, who is a proponent of offensive realism. He is the R. Wendell Harrison Distinguished Service Professor of Political Science at the University of Chicago. He has written several books, including The Tragedy of Great Power Politics.</p> <p>The main idea of his theory is that the international system is anarchic, meaning that there is no higher authority above states(i.e. nation). States are the primary actors in the international system, and they are rational actors that seek to maximize their power. Mearsheimer argues that states are primarily concerned with their own security and survival, and that they will seek to increase their power relative to other states in order to ensure their security. He also argues that states are inherently aggressive and that they will seek to dominate their region in order to ensure their security. His quiet confident about his theory since it can be quiet explanatory to the current situation of the world. Apply it to the relations between China and the US, he believes the shift in balance of power between the two great power causes the tension and the conflict between them.  Looking back, China and the US maintain a good relations from 1991 to roughly 2017 when Trump took office. That's also when the US realized that China is a potential threat to its hegemony. It's not about ideology, but about power. However, both states love to use ideological rhetoric to justify their own stance. That's why people are easily misled by the media and the government and believed that the conflict between China and the US is about democracy and anthoritarianism. Huh, that's an interesting perspective. About China, all the anthoritarianism, the human rights abuse, the censorship, or the US on the other side, the division, the racism, the gun violence, the LGBTQ issues, are all designed to make the other side look bad while maintaining their own image. Feel persuasive. He also gave his prediction, even the sanctions may not be effective, the US has shown its resolution to contain China, capping the advanced technology including semiconductor and AI. We could anticipate that the competitio is gonna last for decades even through the entire 21st century.  There's also some criticism. </p>"},{"location":"Casual%20notes/2024-11-17/","title":"2024 11 17","text":""},{"location":"Casual%20notes/2024-11-17/#another-week","title":"Another week","text":"<p>Time fleets. It's been another week. There's only 20 days left on my exchange bank account. I really want to slow down the pace and enjoy the last few days here.  Ah, I'm feeling a little tired today so I won't write too much. Just a brief update on what's going on lately.</p> <p>I picked up anime again. Can't really remember what triggers me. But I finished Grand Blue and Too Many Losing Heroines! in the past few days. I found it satisfying to binge-watching all 12 episodes in one go. But the bad news is that both animes are so good that I endded up staying up late till 4 or 5 in the morning. This is very bad for my health. Plus the finals are drawing near, I have to get back to a normal schedule. I just added a new doc embedded with bangumi anime colletion. Here is the link.</p>"},{"location":"Misc/Anime-Collection/","title":"Anime Collection","text":""},{"location":"Misc/Music%20of%20the%20Week/","title":"Music of the Week","text":""},{"location":"Misc/Music%20of%20the%20Week/#2024-10-24","title":"2024-10-24","text":"<ul> <li>\u6700\u540e\u7684\u6c34\u65cf\u9986 -- \u88d8\u5fb7</li> <li>SIX -- Live on Broadway 2022</li> <li>November Rain -- Guns N' Roses</li> </ul>"},{"location":"Misc/Music%20of%20the%20Week/#2024-10-30","title":"2024-10-30","text":"<ul> <li>The Heavy Metallic Girl -- Sheena Ringo</li> <li>Identity -- Sheena Ringo</li> </ul>"},{"location":"Misc/Music%20of%20the%20Week/#2024-11-10","title":"2024-11-10","text":"<ul> <li>Dream On -- Aerosmith</li> <li>B\u7ea7\u9ca8\u9c7c -- \u88d8\u5fb7</li> <li>\u6628\u665a\u6211\u68a6\u89c1\u6211\u5b66\u4f1a\u4e86\u6e38\u6cf3 -- \u88d8\u5fb7</li> </ul>"},{"location":"Misc/Music%20of%20the%20Week/#2024-11-17","title":"2024-11-17","text":"<ul> <li>Sweet Child O' Mine</li> </ul>"},{"location":"Misc/mkdocs-configuration/","title":"Current Configuration","text":""},{"location":"Misc/mkdocs-configuration/#current-configuration","title":"Current Configuration","text":"<p>This is a reminder to record the current configuration of the mkdocs.</p>"},{"location":"Misc/mkdocs-configuration/#set-up-envrioment","title":"Set up envrioment","text":"<pre><code>pip install mkdocs\n</code></pre> <pre><code>pip install mkdocs-material\n</code></pre> <pre><code>pip install mkdocs-video\n</code></pre> <pre><code>pip install mkdocs-jupyter\n</code></pre> <pre><code>pip install leafmap\n</code></pre> <pre><code>pip install mkdocs-git-revision-date-localized-plugin\n</code></pre>"},{"location":"Misc/mkdocs-configuration/#mkdocsyml","title":"Mkdocs.yml","text":"<pre><code>site_name: Kevin's Blog\nmarkdown_extensions:\n  - pymdownx.superfences\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.emoji:\n      emoji_index: !!python/name:material.extensions.emoji.twemoji\n      emoji_generator: !!python/name:material.extensions.emoji.to_svg\n  - attr_list\n  - md_in_html\n# below is for the admonition extension\n  - admonition\n  - pymdownx.details\n  - pymdownx.superfences\n  - pymdownx.arithmatex:\n      generic: true\nplugins:\n  - glightbox\n  - search\n  - git-revision-date-localized:\n      enable_creation_date: true  # Show when the file was created\n      locale: 'en'\n      fallback_to_build_date: true  # Use build date if no commit info is found\n  - mkdocs-video:\n      # mark: \"video\"\n      video_autoplay: false\n      video_loop: false\n      video_muted: true\n  - mkdocs-jupyter  \ntheme:\n  name: 'material'\n  font:\n    text: 'Georgia'\n    # text: 'Bookman-Old-Style'\n    code: 'Roboto Mono'\n  icon:\n    logo: simple/keras\n    menu: fontawesome/solid/left-long\n    admonition:\n      note: material/notebook\n      abstract: material/format-list-bulleted\n      info: octicons/info-16\n      tip: material/lightbulb-on\n      success: octicons/check-16\n      question: octicons/question-16\n      warning: octicons/alert-16\n      failure: octicons/x-circle-16\n      danger: octicons/zap-16\n      bug: octicons/bug-16\n      example: material/pencil-outline\n      quote: fontawesome/solid/quote-right\n  features: \n    - content.code.copy\n    - content.code.annotate\n    - content.code.select\n  palette: \n    # Palette toggle for light mode\n    - scheme: default\n      primary: orange\n      toggle:\n        icon: material/brightness-7 \n        name: Switch to dark mode\n\n    # Palette toggle for dark mode\n    - scheme: slate\n      primary: orange\n      toggle: \n        icon: material/brightness-4\n        name: Switch to light mode\nextra_javascript:\n  - https://unpkg.com/leaflet@1.7.1/dist/leaflet.js\n  - javascripts/mathjax.js\n  # - https://polyfill.io/v3/polyfill.min.js?features=es6\n  - javascripts/tex-mml-chtml.js\n  - https://cdn.jsdelivr.net/npm/brython@3.9.5/brython.min.js\n\n\nextra_css:\n  - https://unpkg.com/leaflet@1.7.1/dist/leaflet.css\n  - stylesheets/extra.css\n  - https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css\n  - https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css\n</code></pre>"},{"location":"Misc/nav/","title":"Nav","text":""},{"location":"Misc/nav/#recently","title":"Recently","text":"<ul> <li>github: github</li> <li>Chatbot: Chatgpt or Claude</li> <li>Email: Outlook or Gmail</li> <li>RL platform: <ul> <li>isaac lab</li> <li>aerial-gym</li> </ul> </li> </ul>"},{"location":"Misc/nav/#tools","title":"Tools","text":"<ul> <li>\u6570\u5b66\u7ed8\u56fe\uff1aGeogebra</li> <li>\u8ba1\u7b97\uff1aWolframAlpha</li> <li>online modeling: SculpGL</li> </ul>"},{"location":"Misc/nav/#_1","title":"\u8d44\u6e90","text":"<ul> <li>E-books: Anna's archive</li> <li>\u65e0\u7248\u6743\u56fe\u7247: Unsplash</li> <li>\u6c42\u662f\u6f6e\u8bfe\u7a0b\u4e92\u52a9\u8ba1\u5212: zju-icicles</li> <li>\u6bdb\u5b50\u8d44\u6e90\uff1aRutracker</li> <li>\u58c1\u7eb8\uff1awallhaven</li> <li>AI Navigation: </li> </ul>"},{"location":"Misc/nav/#_2","title":"\u81ea\u5b66","text":"<ul> <li>\u7269\u7406\u6570\u5b66\uff1a\u5c0f\u65f6\u767e\u79d1</li> <li>\u8ba1\u7b97\u673a: csdiyOI Wiki\u83dc\u9e1f\u6559\u7a0b</li> </ul>"},{"location":"Misc/nav/#acg","title":"ACG","text":"<ul> <li>\u8ffd\u756a\uff1a\u55b5\u7269\u6b21\u5143 or \u5907\u7528</li> <li>Manga\uff1a\u5305\u5b50\u6f2b\u753b</li> <li>Pixiv\u955c\u50cf\u7ad9: Pixivic</li> </ul>"},{"location":"Misc/nav/#_3","title":"\u8ffd\u5267","text":"<ul> <li>\u751f\u6d3b\u5927\u7206\u70b8</li> </ul>"},{"location":"Misc/nav/#mod","title":"\u6e38\u620fmod","text":"<ul> <li>nexusmods</li> <li>playground</li> </ul>"},{"location":"Misc/nav/#_4","title":"\u5f71\u89c6\u8bc4\u5206\u7f51\u7ad9","text":"<ul> <li>imdb</li> <li>rottentomatoes</li> <li>bangumi</li> </ul>"},{"location":"Misc/nav/#_5","title":"\u6e38\u620f\u8bc4\u5206\u7f51\u7ad9","text":"<ul> <li>metacritic</li> <li>opencritic</li> <li>ign</li> </ul>"},{"location":"Misc/nav/#wikis","title":"wikis!","text":"<ul> <li>musclewiki</li> <li>Isaac Wiki</li> </ul>"},{"location":"Misc/nav/#6","title":"\u8857\u97386","text":"<ul> <li>\u5e27\u6570: ultimateframedata</li> <li>Capcom SF6: \u5b98\u7f51</li> </ul>"},{"location":"Misc/reminder/","title":"REMINDERS","text":""},{"location":"Misc/reminder/#reminders","title":"REMINDERS","text":""},{"location":"Misc/reminder/#chatanywhere-api-key","title":"Chatanywhere api key:","text":"<pre><code>sk-gYQITBKxjZJWhb5cCfA8Y0ckZjffg5XJDGRbvMe4zIDAUx9V\n</code></pre>"},{"location":"Misc/reminder/#accessible-admonitions","title":"Accessible admonitions:","text":"note abstract info question tip warning failure danger bug example quote success"},{"location":"Notes/RL/Actor-Critic/","title":"Actor Critic","text":""},{"location":"Notes/RL/Actor-Critic/#actor-critic","title":"Actor-Critic","text":"<p>\u672c\u8d28\u4e0a\u662f\u57fa\u4e8e\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u4f46\u989d\u5916\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u7b56\u7565\u7684\u5b66\u4e60\u6548\u679c\u3002</p>"},{"location":"Notes/RL/Actor-Critic/#takeaways","title":"Takeaways","text":"<ul> <li> <p>baseline function: \u8003\u8651\u7b56\u7565\u7684\u68af\u5ea6\uff1a</p> \\[ g = E\\left[\\sum_{t=0}^T\\psi_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right] \\] <p>\u5728REINFORCE\u4e2d\uff0c\\(\\psi_t = G_t\\)\uff0c\u5373MC\u7684\u56de\u62a5\u3002\u800c\u5728Actor-Critic\u4e2d\uff0c\u53ef\u4ee5\u9009\u53d6\u4f18\u52bf\u51fd\u6570\\(A^{\\pi_\\theta}(s_t,a_t)\\)\u6216\u8005TD error \\(r_t+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_t)\\)\u4f5c\u4e3abaseline function,\u3002</p> \u4e3a\u4ec0\u4e48\u9009V(s)? <p>\u7531\u4e8e\u5f15\u5165b(s)\u4e0d\u4f1a\u6539\u53d8\u671f\u671b\uff0c\u53ea\u4f1a\u5f71\u54cd\u65b9\u5dee\uff0c\u6240\u4ee5\u53ef\u4ee5\u9009\u62e9\u5408\u9002\u7684baseline function\u6765\u51cf\u5c0f\u65b9\u5dee\uff0c\u800c\u9009\u62e9V(s)\u4f5c\u4e3abaseline function\uff0c\u5b9e\u9645\u4e0a\u66f4\u65b0\u7684\u5c31\u662f\u4f18\u52bf\u51fd\u6570A(s,a)\uff0c\u8fd9\u662f\u5bf9\u52a8\u4f5c\u76f8\u5bf9\u4e8e\u57fa\u51c6\u7684\u8bc4\u4ef7.</p> \u4e3a\u4ec0\u4e48b(s)\u4e0d\u4f1a\u6539\u53d8\u671f\u671b <p>\u8981\u8bc1\u660e\uff1a</p> \\[ b(s) \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\right] = 0 \\] <p>\u6b65\u9aa41\uff1a\u7b56\u7565\u7684\u5f52\u4e00\u5316\u6027\u8d28 \u56e0\u4e3a \\(\\pi_{\\theta}(a|s)\\) \u662f\u4e00\u4e2a\u6982\u7387\u5206\u5e03\uff0c\u6240\u4ee5\u6ee1\u8db3\u4ee5\u4e0b\u5f52\u4e00\u5316\u6761\u4ef6\uff1a</p> \\[ \\sum_{a} \\pi_{\\theta}(a|s) = 1 \\] <p>\u5bf9\u8be5\u7b49\u5f0f\u4e24\u8fb9\u5173\u4e8e\u53c2\u6570 \\(\\theta\\) \u6c42\u5bfc\uff1a</p> \\[ \\nabla_{\\theta} \\sum_{a} \\pi_{\\theta}(a|s) = \\nabla_{\\theta} 1 = 0 \\] <p>\u7531\u4e8e\u6c42\u548c\u7b26\u53f7\u4e0e\u68af\u5ea6\u8fd0\u7b97\u53ef\u4ee5\u4e92\u6362\uff0c\u56e0\u6b64\u4e0a\u5f0f\u53ef\u4ee5\u5199\u4e3a\uff1a</p> \\[ \\sum_{a} \\nabla_{\\theta} \\pi_{\\theta}(a|s) = 0 \\] <p>\u6b65\u9aa42\uff1a\u94fe\u5f0f\u6cd5\u5219</p> <p>\u5229\u7528\u94fe\u5f0f\u6cd5\u5219\uff0c\u6211\u4eec\u5bf9 \\(\\nabla_{\\theta} \\pi_{\\theta}(a|s)\\) \u8fdb\u884c\u5c55\u5f00\uff1a</p> \\[ \\nabla_{\\theta} \\pi_{\\theta}(a|s) = \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\] <p>\u5c06\u5176\u4ee3\u5165\u524d\u9762\u7684\u7b49\u5f0f\uff1a</p> \\[ \\sum_{a} \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) = 0 \\] <p>\u8fd9\u610f\u5473\u7740\uff1a</p> \\[ \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\right] = \\sum_{a} \\pi_{\\theta}(a|s) \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) = 0 \\] <p>\u6b65\u9aa43\uff1a\u57fa\u7ebf\u51fd\u6570\u4e0d\u5f71\u54cd\u671f\u671b</p> <p>\u6211\u4eec\u73b0\u5728\u8003\u8651\u5f15\u5165\u57fa\u7ebf \\(b(s)\\)\uff0c\u5e76\u8bc1\u660e\u5176\u4e0d\u4f1a\u6539\u53d8\u671f\u671b\u3002\u56e0\u4e3a \\(b(s)\\) \u662f\u4e0e\u72b6\u6001 \\(s\\) \u76f8\u5173\u7684\u5e38\u6570\uff0c\u6240\u4ee5\u5b83\u53ef\u4ee5\u88ab\u63d0\u5230\u671f\u671b\u5916\uff1a</p> \\[ b(s) \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\right] = b(s) \\cdot 0 = 0 \\] <p>\u56e0\u6b64\uff0c\u5f15\u5165\u57fa\u7ebf\u51fd\u6570 \\(b(s)\\) \u4e0d\u4f1a\u6539\u53d8\u7b56\u7565\u68af\u5ea6\u7684\u671f\u671b\u503c\u3002</p> <p>\u7531\u4e8e</p> \\[ \\mathbb{E}_{a \\sim \\pi_\\theta} \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\right] = 0 \\] <p>\u6211\u4eec\u8bc1\u660e\u4e86\u5f15\u5165\u57fa\u7ebf \\(b(s)\\) \u4e0d\u4f1a\u6539\u53d8\u7b56\u7565\u68af\u5ea6\u7684\u671f\u671b\uff0c\u53ea\u4f1a\u5bf9\u65b9\u5dee\u4ea7\u751f\u5f71\u54cd\u3002</p> <ul> <li>critic \u7684\u66f4\u65b0\u65b9\u5f0f\uff1a   define loss function:</li> </ul> \\[ L(w) = \\frac{1}{2}(r+\\gamma V_w(s')-V_w(s))^2 \\] <p>update rule: Do as what we do in DQN: treat \\(r+\\gamma V_w(s')\\) as target, generate from target network, and minimize the loss function.</p> \\[ \\delta_w L(w) = -\\nabla_w L(w) = (r+\\gamma V_w(s')-V_w(s))\\nabla_w V_w(s) \\] <p>The benefit of using target network is to stabilize the training process.</p> </li> </ul>"},{"location":"Notes/RL/Policy-Gradient/","title":"Policy Gradient","text":""},{"location":"Notes/RL/Policy-Gradient/#policy-gradient","title":"Policy-Gradient","text":"<ul> <li>\u53c2\u6570\u5316\u7b56\u7565\uff1a(parameterized policy) </li> <li> <p>\u76ee\u6807\u51fd\u6570: (metric)</p> \\[ J(\\theta) = E_{s_0}[V^{\\pi_{\\theta}}(s_0)] \\] <p>where \\(s_0\\) represents the initial state, and \\(V^{\\pi_{\\theta}}(s_0)\\) is the value function of the policy \\(\\pi_{\\theta}\\) defined by </p> \\[ V^{\\pi_{\\theta}}(s_0) = E_{\\pi}[G_t|S_t = s_0] \\quad or \\quad V^{\\pi_{\\theta}}(s_0) = E_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)|s_0] \\] \u8bbf\u95ee\u5206\u5e03\uff08on-policy distribution\uff09 <p>Denoted by \\(\\nu^\\pi\\)(s) or simply \\(d(s)\\). It is the distribution of states encountered by the agent when following policy \\(\\pi\\). It is defined by</p> \\[ \\nu^\\pi(s) = (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^tP^\\pi_t(s) \\] <p>namely, the probability of visiting state s at time t under policy \\(\\pi\\). (1-\\(\\gamma\\)) is the normalization factor.</p> <p>consider the following equation:</p> \\[ \\sum_{s_i \\in S} \\gamma^t P^\\pi_t(s_i) = \\gamma^t \\] \\[ \\sum_{s_i \\in S}\\sum_{t=0}^\\infty \\gamma^t P^\\pi_t(s_i) = \\sum_t^\\infty \\gamma^t = \\frac{1}{1-\\gamma} \\] <p>besides, we can define occupancy measure by:</p> \\[ \\rho^\\pi(s,a) = (1-\\gamma)\\sum_{t=0}^\\infty \\gamma^t P^\\pi_t(s,a) = \\nu^\\pi(s) \\pi(a|s) \\] <p>TODO: -  frequency of visiting?</p> </li> <li> <p>Intuition of policy gradient:</p> <p></p> </li> <li> <p>\u8bc1\u660e\\(\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\pi_{\\theta}}\\left[Q^{\\pi_{\\theta}}\\nabla_\\theta\\log\\pi_{\\theta}(a|s) \\right]\\) </p> <p>Intuition: <code>Q</code> denotes the reward of taking action <code>a</code> in state <code>s</code> under policy \\(\\pi_{\\theta}\\), and \\(\\nabla_\\theta\\log\\pi_{\\theta}(a|s)\\) denotes the gradient of the log probability of taking action <code>a</code> in state <code>s</code> under policy \\(\\pi_{\\theta}\\). The expectation of the product of these two terms is proportional to the gradient of the objective function \\(J(\\theta)\\). </p> Recall the expreesion below \\[ E\\left[Q^{\\pi_\\theta}\\pi_\\theta(a|s) \\right] \\] <p>Q: What does this expression represent? A: Actually, is't the <code>state value</code> of <code>s</code></p> <p>Proof:</p> \\[ \\begin{aligned} J(\\theta) = E_{s_0}[V^{\\pi_{\\theta}}(s_0)] \\end{aligned} \\] <p>Started from the expression inside the expectation which is the value function of the policy of initial state \\(s_0\\) under policy \\(\\pi_{\\theta}\\): </p> \\[ \\begin{aligned} \\nabla_\\theta V^{\\pi_\\theta}(s) &amp;= \\nabla_\\theta(\\sum_{a \\in A}\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a)) \\\\ \\text{Apply the chain rule: } \\\\ &amp;= \\sum_{a \\in A}\\nabla_\\theta\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a) + \\sum_{a \\in A}\\pi_\\theta(a|s)\\nabla_\\theta Q^{\\pi_\\theta}(s,a)\\\\ \\text{Rewrite the Q function: } \\\\ &amp;= \\sum_{a \\in A}\\nabla_\\theta\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a) + \\sum_{a \\in A}\\pi_\\theta(a|s)\\nabla_\\theta \\sum_{s',r}P(s',r|s,a)(r + \\gamma V^{\\pi_\\theta}(s'))\\\\ \\text{Exclude the first term since it's not the function of $\\theta$: } \\\\ &amp;= \\sum_{a \\in A}\\nabla_\\theta\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a) + \\sum_{a \\in A}\\pi_\\theta(a|s)\\nabla_\\theta \\sum_{s',r}P(s',r|s,a)\\gamma V^{\\pi_\\theta}(s')\\\\ \\text{Then we simply marginalize the reward term: r} \\\\ &amp; = \\sum_{a \\in A}\\nabla_\\theta\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a) + \\sum_{a \\in A}\\pi_\\theta(a|s)\\gamma \\sum_{s'}P(s'|s,a)\\nabla_\\theta V^{\\pi_\\theta}(s')\\\\ \\end{aligned} \\] <p>To simplify the expression, we can denote the first term as \\(\\Phi(s)\\) and define \\(d^{\\pi_\\theta}(s \\to x,k)\\) as the probability of transitioning from state <code>s</code> to state <code>x</code> in <code>k</code> steps under policy \\(\\pi_\\theta\\). Then we continue our derivation:</p> \\[ \\begin{aligned} \\nabla_\\theta V^{\\pi_\\theta}(s) &amp;= \\Phi(s) + \\gamma \\sum_{a \\in A}\\pi_\\theta(a|s)\\sum_{s'}P(s'|s,a)\\nabla_\\theta V^{\\pi_\\theta}(s')\\\\ &amp; = \\Phi(s) + \\gamma \\sum_{a}\\sum{s'}\\pi_\\theta(a|s)P(s'|s,a)\\nabla_\\theta V^{\\pi_\\theta}(s')\\\\ \\text{Simplify the expression using our definition above: } \\\\ &amp; = \\Phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\to s',1)\\nabla_\\theta V^{\\pi_\\theta}(s')\\\\ \\text{Note that the above expression is a recursive form: } \\\\ &amp; = \\Phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\to s',1)\\left(\\Phi(s') + \\gamma \\sum_{s''}d^{\\pi_\\theta}(s' \\to s'',1)\\nabla_\\theta V^{\\pi_\\theta}(s'')\\right)\\\\ \\text{Continue the recursion: } \\\\ &amp; = \\Phi(s) + \\gamma \\sum_{s'}d^{\\pi_\\theta}(s \\to s',1)\\left(\\Phi(s') + \\gamma \\sum_{s''}d^{\\pi_\\theta}(s' \\to s'',1)\\left(\\Phi(s'') + \\gamma \\sum_{s'''}d^{\\pi_\\theta}(s'' \\to s''',1)\\nabla_\\theta V^{\\pi_\\theta}(s''')\\right)\\right)\\\\ &amp; = \\cdots \\text{Since it can visit every state in the MDP, the recursion be shorten to: } \\\\ &amp; = \\sum_{x \\in S}\\sum_{k=0}^{\\infty}\\gamma^k d^{\\pi_\\theta}(s \\to x,k)\\Phi(x)\\\\ \\end{aligned} \\] <p>Then we define \\(\\eta(s) = E\\left[\\sum_{k=0}^{\\infty}\\gamma^k d^{\\pi_\\theta}(s_0 \\to s,k)\\right]\\). We can finally retract our gaze to the objective function \\(J(\\theta)\\):</p> \\[ \\begin{aligned} \\nabla_\\theta J(\\theta) &amp;= \\nabla_\\theta E_{s_0}[V^{\\pi_\\theta}(s_0)]\\\\ \\text{Move the gradient inside the expectation: } \\\\ &amp;= E_{s_0}[\\nabla_\\theta V^{\\pi_\\theta}(s_0)]\\\\ \\text{Apply the expression we derived above: } \\\\ &amp;= E_{s_0}\\left[\\sum_{x \\in S}\\sum_{k=0}^{\\infty}\\gamma^k d^{\\pi_\\theta}(s_0 \\to x,k) \\Phi(x)\\right]\\\\ &amp;= \\sum_{x \\in S} E_{s_0}[\\sum_{k=0}^{\\infty}\\gamma^kd^{\\pi_\\theta}(s_0 \\to x,k)]\\Phi(x)\\\\ &amp;= \\sum_{x \\in S} \\eta(x)\\Phi(x)\\\\ &amp;= \\sum_{s} \\eta(s)\\Phi(s)\\\\ &amp;= \\left(\\sum_{s} \\eta(s)\\right)\\sum_s \\frac{\\eta(s)}{\\sum_s \\eta(s)}\\Phi(s)\\\\ &amp;\\propto \\sum_s \\frac{\\eta(s)}{\\sum_s \\eta(s)}\\Phi(s)\\\\ &amp;= \\sum_s \\nu^{\\pi_\\theta}(s)\\sum_a \\nabla_\\theta \\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a)\\\\ q.e.d \\end{aligned}_ \\] <p>where \\(\\nu^{\\pi_\\theta}(s)\\) is the on-policy distribution of states under policy \\(\\pi_\\theta\\).11111</p> </li> </ul>"},{"location":"Notes/RL/TRPO/","title":"TRPO","text":""},{"location":"Notes/RL/TRPO/#trpotrust-region-policy-optimization","title":"TRPO(trust region policy optimization)","text":"Fisher Information <p>Fisher information measures the amount of information that an observable random variable X carries about an unknown parameter \\(\\theta\\) upon which the probability of X depends. It's defined as the expected value of the square of the score function, which is the first derivative of the log-likelihood of X with respect to \\(\\theta\\).</p> <p>For a random variable \\(X\\) with probability density function \\(p(x|\\theta)\\), the Fisher information is defined as:</p> \\[ I(\\theta) = E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log p(x|\\theta)\\right)^2\\right] \\] <p>and for a sample of \\(n\\) independent distributed random variables \\(X_1, X_2, \\ldots, X_n\\), the Fisher information is:</p> \\[ I(\\theta) = \\sum_{i=1}^n E\\left[\\left(\\frac{\\partial}{\\partial \\theta} \\log p(x_i|\\theta)\\right)^2\\right] \\] <p>Intuition:</p> <ul> <li>Recall the likelihood function \\(L(\\theta) = \\prod_{i=1}^{n} p(x_i|\\theta)\\), and score function \\(S(\\theta) = \\frac{\\partial}{\\partial \\theta} \\log L(\\theta)\\). So Fisher information is the expected value of the square of the score function. </li> <li>Fisher information measures the amount of information that the data carries about the parameter \\(\\theta\\). The larger the Fisher information, the more information the data carries about \\(\\theta\\).</li> </ul> KL divergence <p>Kullback\u2013Leibler divergence measures how one probability distribution  P differs from a second reference distribution  Q. It's calculated as:</p> \\[ D_{KL}(p||q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)} \\] <p>or for continuous distributions:</p> \\[ D_{KL}(p||q) = \\int_{-\\infty}^\\infty p(x) \\log \\frac{p(x)}{q(x)} dx \\] <p>Intuition:</p> <ul> <li>If \\(P = Q\\), then \\(D_{KL}(p||q) = 0\\) </li> <li>The larger the divergence, the more different \ud835\udc43 is from Q, meaning there's more \"surprise\" in using Q when the true distribution is P.</li> </ul> <p>below is a step by step derivation of the KL divergence between two Gaussian distributions:</p> \\[ \\begin{aligned} D_{KL}(N(\\mu_0, \\sigma_0^2) || N(\\mu_1, \\sigma_1^2)) &amp;= \\int_{-\\infty}^{\\infty} N(\\mu_0, \\sigma_0^2) \\log \\frac{N(\\mu_0, \\sigma_0^2)}{N(\\mu_1, \\sigma_1^2)} dx \\\\ &amp;= \\int_{-\\infty}^{\\infty} N(\\mu_0, \\sigma_0^2) \\log \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma_0^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right)} dx \\\\ &amp;= \\int_{-\\infty}^{\\infty} N(\\mu_0, \\sigma_0^2) \\left(\\log \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} - \\frac{(x-\\mu_0)^2}{2\\sigma_0^2} - \\log \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} + \\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right) dx \\\\ &amp;= \\log \\frac{\\sigma_1}{\\sigma_0} + \\frac{\\sigma_0^2 + (\\mu_0 - \\mu_1)^2}{2\\sigma_1^2} - \\frac{1}{2} \\end{aligned} \\] Hesse-Matrix <p>Hessian matrix is a square matrix of second-order partial derivatives of a scalar-valued function. It describes the local curvature of a function of many variables. </p> <p>For a function \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\), the Hessian matrix is defined as:</p> \\[ H(f) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] <p>The Hessian matrix is symmetric if the second-order partial derivatives are continuous in a neighborhood of the point.</p>"},{"location":"Notes/RL/TRPO/#intuitively","title":"Intuitively","text":"<p>Compared to the network in deep learning, the policy network in reinforcement learning is a function that maps states to actions. Consider how steps affect these two networks. for the former one, let's say, supervised learning, the goal is to minimize the loss function. If we took too large a step, it could be harder to converge. However, for the latter one, the policy network, the networks maps states to actions, and the goal is to maximize the reward. If we took too large a step, and the policy may lead to a completely diffrent state and then, trajectory, like ripple effect result in a sudden and significant decline in performance. So, TRPO was proposed to solve this problem.</p>"},{"location":"Notes/RL/TRPO/#policy-objective","title":"\u7b56\u7565\u76ee\u6807\uff08Policy Objective\uff09","text":"<p>Let's say our current policy is \\(\\pi_{\\theta}\\), and we want to find a new policy \\(\\pi_{\\theta'}\\) which makes \\(J(\\theta') \\geq J(\\theta)\\). </p> \\[ \\begin{aligned} J(\\theta) &amp;=E_{s_{0}}[V^{\\pi_{\\theta}}(s_{0})]\\\\           &amp;=E_{\\pi_\\theta'}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}V^{\\pi_{\\theta}}(s_{t}) - \\sum_{t=1}^\\infty \\gamma^{t}V^{\\pi_{\\theta}}(s_{t})\\right]\\\\           &amp;= -E_{\\pi_\\theta'}\\left[\\sum_{t=0}^\\infty \\gamma^{t} (\\gamma V^{\\pi_{\\theta}}(s_{t}) - V^{\\pi_{\\theta}}(s_{t}))\\right]\\\\ \\end{aligned} \\] <p>\u5728\u4e0a\u5f0f1\u52302\u8fc7\u7a0b\u4e2d\uff0c\u5c06\u4e0b\u6807\u8fdb\u884c\u4e86\u66ff\u6362\uff0c\u4ece\\(s_{0}\\)\u5230\\(\\pi_\\theta'\\) \u8fd9\u6837\u505a</p> <ul> <li>\u53ef\u4ee5\u662f\u56e0\u4e3a\uff1a\\(E_{s_{0}}[V^{\\pi_\\theta}(s_0)]\\) \u8868\u793a\u7684\u662f\u5728\u521d\u59cb\u72b6\u6001\\(s_{0}\\)\u4e0b\u7684\u4ef7\u503c\u51fd\u6570\u7684\u503c\uff0c\u800c\u7531\u4e8e\u521d\u59cb\u72b6\u6001\u7684\u5206\u5e03\u4e0e\u7b56\u7565\u65e0\u5173/\u72ec\u7acb\uff08\u901a\u5e38\u662f\u6839\u636e\u5b9e\u9a8c\u8bbe\u5b9a\u7684\u56fa\u5b9a\u503c\u6216\u8005\u5206\u5e03\uff09\uff0c\u6240\u4ee5\u53ef\u4ee5\u66ff\u6362\u4e3a\\(\\pi_\\theta'\\)\uff0c\u5373\u5728\\(\\pi_\\theta'\\)\u4e0b\u7684\u4ef7\u503c\u51fd\u6570\u7684\u503c\u3002\uff08PS: \\(V^{\\pi_\\theta}(s)\\)\u5c31\u662f\u4e00\u4e2a\u51fd\u6570\uff0c\u4e0e\u5f53\u524d\u7684\u7b56\u7565\u6ca1\u6709\u5173\u7cfb\uff0c\u8f93\u5165\u4e00\u4e2as\uff0c\u7ed9\u51fa\u4e00\u4e2avalue\uff0cthat\u2018s it\uff09</li> <li>\u4fbf\u4e8e\u8fdb\u4e00\u6b65\u8bc4\u4f30\u7b56\u7565\u7684\u4f18\u52a3\uff0c\u5373\u901a\u8fc7\u6bd4\u8f83\u4e24\u4e2a\u7b56\u7565\u7684\u671f\u671b\u503c\uff0c\u6765\u5224\u65ad\u54ea\u4e2a\u7b56\u7565\u66f4\u597d\u3002</li> </ul> <p>\u7ee7\u7eed\u63a8\u5bfc</p> \\[ \\begin{aligned} J(\\theta') - J(\\theta) &amp;= E_{s_{0}}[V^{\\pi_{\\theta'}}(s_{0})] - E_{s_{0}}[V^{\\pi_{\\theta}}(s_{0})]  \\\\ &amp;= E_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_t,a_t)\\right]+E_{\\pi_\\theta'}\\left[\\sum_{t=0}^\\infty \\gamma^{t} (\\gamma V^{\\pi_{\\theta}}(s_{t}) - V^{\\pi_{\\theta}}(s_{t}))\\right]\\\\ &amp; = E_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left[r(s_t,a_t)+\\gamma V^{\\pi_{\\theta}}(s_{t})-V^{\\pi_{\\theta}}(s_{t})\\right]\\right]\\\\ \\end{aligned} \\] <p>\u5c06\u65f6\u5e8f\u5dee\u5206\u6b8b\u5dee\uff08TD residual\uff09\u5b9a\u4e49\u4e3a\u4f18\u52bf\u51fd\u6570\\(A^{\\pi_{\\theta}}(s,a)\\)\uff0c\u90a3\u4e48\u4e0a\u5f0f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7b80\u5316\u4e3a\uff1a</p> \\[ \\begin{aligned} J(\\theta') - J(\\theta) &amp;= E_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}A^{\\pi_{\\theta}}(s_t,a_t)\\right]\\\\ &amp;= \\sum_{t=0}^\\infty \\gamma^{t}E_{s_t \\sim P_t^{\\pi_{\\theta'}}}E_{a_t \\sim \\pi_\\theta'(a|s_t)}[A^{\\pi_{\\theta}}(s_t,a_t)]\\\\ &amp;= \\frac{1}{1-\\gamma}E_{s \\sim \\nu^{\\pi_{\\theta'}}}E_{a \\sim \\pi_\\theta'(a|s)}[A^{\\pi_{\\theta}}(s,a)]\\\\ \\end{aligned} \\]"},{"location":"Notes/RL/aerialgym/","title":"Aerial Gym Simulator","text":""},{"location":"Notes/RL/aerialgym/#aerial-gym-simulator","title":"Aerial Gym Simulator","text":""},{"location":"Notes/RL/aerialgym/#environment-settings","title":"Environment settings","text":"<p>Under the <code>config/env_config</code> folder, can set up the environment settings for the training including: - default env number, action number - reset on collision - bounding box - assets to include</p> <p>Under the <code>config/asset_config</code> folder, can set up the assets</p>"},{"location":"Notes/RL/aerialgym/#task-settings","title":"Task settings","text":""},{"location":"Notes/RL/isaaclab/","title":"Isaaclab","text":""},{"location":"Notes/RL/isaaclab/#tutorial","title":"Tutorial","text":"Customize and parse cli arguments <pre><code>import argparse\n\nfrom omni.isaac.lab.app import AppLauncher\n\n# create argparser\nparser = argparse.ArgumentParser(description=\"Tutorial on running IsaacSim via the AppLauncher.\")\nparser.add_argument(\"--size\", type=float, default=1.0, help=\"Side-length of cuboid\")\n# SimulationApp arguments https://docs.omniverse.nvidia.com/py/isaacsim/source/extensions/omni.isaac.kit/docs/index.html?highlight=simulationapp#omni.isaac.kit.SimulationApp\nparser.add_argument(\n    \"--width\", type=int, default=1280, help=\"Width of the viewport and generated images. Defaults to 1280\"\n)\nparser.add_argument(\n    \"--height\", type=int, default=720, help=\"Height of the viewport and generated images. Defaults to 720\"\n)\n\n# append AppLauncher cli args\nAppLauncher.add_app_launcher_args(parser)\n# parse the arguments\nargs_cli = parser.parse_args()\n# launch omniverse app\napp_launcher = AppLauncher(args_cli)\nsimulation_app = app_launcher.app\n</code></pre> Now we can access the cli args through the <code>args_cli</code> object. For example, <code>args_cli.size</code> will return the value of the <code>size</code> argument.  InteractiveScene Direct Workflow RL Env Register Custom RL Env Example"},{"location":"Notes/RL/isaaclab/#test-environment","title":"Test Environment","text":"<p>Zero-action agent </p><pre><code>./isaaclab.sh -p source/standalone/environments/zero_agent.py --task $registered_env_num$ --num_envs 32\n</code></pre> Random agent <pre><code>./isaaclab.sh -p source/standalone/environments/random_agent.py --task $registered_env_name$ --num_envs 32\n</code></pre>"},{"location":"Notes/RL/resources/","title":"Resources","text":""},{"location":"Notes/RL/resources/#rl-resources","title":"RL Resources","text":"<ol> <li>textbook: WU shiyu zhao or Dive into RL</li> <li>course media: WU shiyu zhao</li> <li>notes: MorphoTheRain</li> <li>"},{"location":"Notes/d2l/linear_network/","title":"Linear network","text":""},{"location":"Notes/d2l/linear_network/#linear-regression","title":"Linear Regression","text":"<p>notions:</p> <ul> <li>\\(w \\implies weight ,w \\in \\mathbb{R}^d\\) \u6743\u91cd</li> <li>\\(\\hat{y}\\implies \\space predicted \\space value\\) \u9884\u6d4b\u503c</li> <li>\\(X \\implies feature\\space set\uff0cX \\in \\mathbb{R}^{n\\times d}\\) \u00a0\u542b\u6709n\u4e2a\u6837\u672c\u4ee5d\u4e2a\u7279\u5f81\u7684\u7279\u5f81\u96c6\u5408</li> <li>\\(x\\implies \u5355\u4e2a\u6837\u672c\u7684\u7279\u5f81\\space ,x \\in \\mathbb{R}^d\\)</li> <li>\\(y \\implies label\\) \u6807\u7b7e\uff0c\u5373\u771f\u5b9e\u503c</li> </ul> <p>\u4e8e\u662f\u6211\u4eec\u53ef\u4ee5\u7b80\u6d01\u5730\u8868\u793a\u9884\u6d4b\u7ed3\u679c\u548c\u6a21\u578b\uff1a</p> \\[ \\begin{aligned} \\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b. \\\\ \\\\ {\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b \\end{aligned} \\]"},{"location":"Notes/d2l/linear_network/#loss-function","title":"Loss Function \u635f\u5931\u51fd\u6570","text":"<p>\u635f\u5931\u51fd\u6570\uff08loss function\uff09\u80fd\u591f\u91cf\u5316\u76ee\u6807\u7684\u5b9e\u9645\u503c\u4e0e\u9884\u6d4b\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002 \u901a\u5e38\u6211\u4eec\u4f1a\u9009\u62e9\u975e\u8d1f\u6570\u4f5c\u4e3a\u635f\u5931\uff0c\u4e14\u6570\u503c\u8d8a\u5c0f\u8868\u793a\u635f\u5931\u8d8a\u5c0f\uff0c\u5b8c\u7f8e\u9884\u6d4b\u65f6\u7684\u635f\u5931\u4e3a0\u3002 \u56de\u5f52\u95ee\u9898\u4e2d\u6700\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\u662f\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\u3002</p> \u4e3a\u4ec0\u4e48\u53ef\u4ee5\u7528\u5e73\u65b9\u8bef\u5dee\u51fd\u6570\u4f5c\u4e3a\u635f\u5931\u51fd\u6570 <p>\u6211\u4eec\u5047\u8bbe\u4e86\u89c2\u6d4b\u4e2d\u5305\u542b\u566a\u58f0\uff0c\u5176\u4e2d\u566a\u58f0\u670d\u4ece\u6b63\u6001\u5206\u5e03\u3002 \u566a\u58f0\u6b63\u6001\u5206\u5e03\u5982\u4e0b\u5f0f:</p> \\[y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon,\\] <p>\u5176\u4e2d\uff0c\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\)\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u4eec\u73b0\u5728\u53ef\u4ee5\u5199\u51fa\u901a\u8fc7\u7ed9\u5b9a\u7684\\(\\mathbf{x}\\)\u89c2\u6d4b\u5230\u7279\u5b9a\\(y\\)\u7684\u4f3c\u7136\uff08likelihood\uff09\uff1a</p> \\[P(y \\mid \\mathbf{x}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{1}{2 \\sigma^2} (y - \\mathbf{w}^\\top \\mathbf{x} - b)^2\\right).\\] <p>\u73b0\u5728\uff0c\u6839\u636e\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u6cd5\uff0c\u53c2\u6570\\(\\mathbf{w}\\)\u548c\\(b\\)\u7684\u6700\u4f18\u503c\u662f\u4f7f\u6574\u4e2a\u6570\u636e\u96c6\u7684\u4f3c\u7136\u6700\u5927\u7684\u503c\uff1a</p> \\[P(\\mathbf y \\mid \\mathbf X) = \\prod_{i=1}^{n} p(y^{(i)}|\\mathbf{x}^{(i)}).\\] <p>\u6839\u636e\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u6cd5\u9009\u62e9\u7684\u4f30\u8ba1\u91cf\u79f0\u4e3a\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u91cf\u3002 \u867d\u7136\u4f7f\u8bb8\u591a\u6307\u6570\u51fd\u6570\u7684\u4e58\u79ef\u6700\u5927\u5316\u770b\u8d77\u6765\u5f88\u56f0\u96be\uff0c \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u76ee\u6807\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4f3c\u7136\u5bf9\u6570\u6765\u7b80\u5316\u3002 \u7531\u4e8e\u5386\u53f2\u539f\u56e0\uff0c\u4f18\u5316\u901a\u5e38\u662f\u8bf4\u6700\u5c0f\u5316\u800c\u4e0d\u662f\u6700\u5927\u5316\u3002 \u6211\u4eec\u53ef\u4ee5\u6539\u4e3a\u6700\u5c0f\u5316\u8d1f\u5bf9\u6570\u4f3c\u7136\\(-\\log P(\\mathbf y \\mid \\mathbf X)\\)\u3002 \u7531\u6b64\u53ef\u4ee5\u5f97\u5230\u7684\u6570\u5b66\u516c\u5f0f\u662f\uff1a</p> \\[-\\log P(\\mathbf y \\mid \\mathbf X) = \\sum_{i=1}^n \\frac{1}{2} \\log(2 \\pi \\sigma^2) + \\frac{1}{2 \\sigma^2} \\left(y^{(i)} - \\mathbf{w}^\\top \\mathbf{x}^{(i)} - b\\right)^2.\\] <p>\u73b0\u5728\u6211\u4eec\u53ea\u9700\u8981\u5047\u8bbe\\(\\sigma\\)\u662f\u67d0\u4e2a\u56fa\u5b9a\u5e38\u6570\u5c31\u53ef\u4ee5\u5ffd\u7565\u7b2c\u4e00\u9879\uff0c \u56e0\u4e3a\u7b2c\u4e00\u9879\u4e0d\u4f9d\u8d56\u4e8e\\(\\mathbf{w}\\)\u548c\\(b\\)\u3002 \u73b0\u5728\u7b2c\u4e8c\u9879\u9664\u4e86\u5e38\u6570\\(\\frac{1}{\\sigma^2}\\)\u5916\uff0c\u5176\u4f59\u90e8\u5206\u548c\u524d\u9762\u4ecb\u7ecd\u7684\u5747\u65b9\u8bef\u5dee\u662f\u4e00\u6837\u7684\u3002 \u5e78\u8fd0\u7684\u662f\uff0c\u4e0a\u9762\u5f0f\u5b50\u7684\u89e3\u5e76\u4e0d\u4f9d\u8d56\u4e8e\\(\\sigma\\)\u3002 \u56e0\u6b64\uff0c\u5728\u9ad8\u65af\u566a\u58f0\u7684\u5047\u8bbe\u4e0b\uff0c\u6700\u5c0f\u5316\u5747\u65b9\u8bef\u5dee\u7b49\u4ef7\u4e8e\u5bf9\u7ebf\u6027\u6a21\u578b\u7684\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\u3002</p> <p>\u5f53\u6837\u672c\\(i\\)\u7684\u9884\u6d4b\u503c\u4e3a\\(\\hat{y}^{(i)}\\)\uff0c\u5176\u76f8\u5e94\u7684\u771f\u5b9e\u6807\u7b7e\u4e3a\\(y^{(i)}\\)\u65f6\uff0c \u5e73\u65b9\u8bef\u5dee\u53ef\u4ee5\u5b9a\u4e49\u4e3a\u4ee5\u4e0b\u516c\u5f0f\uff1a</p> \\[l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.\\] <p>\u4e3a\u4e86\u5ea6\u91cf\u6a21\u578b\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6240\u6709\u6240\u6709\u6837\u672c\u7684\u635f\u5931\u6c42\u5e73\u5747\u3002\uff08\u5373\u5bf9\u4e0a\u5f0f\u6c42\u5747\u503c\uff09</p> \\[L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\\] <p>\u4e5f\u53ef\u4ee5\u5199\u6210\u77e9\u9635\u7684\u5f62\u5f0f\uff1a</p> \\[L(\\mathbf{w}, b) = \\frac{1}{2n} \\| \\mathbf{X}\\mathbf{w} + b - \\mathbf{y} \\| ^2.\\] Practice <p>\u6211\u4eec\u7684\u9884\u6d4b\u95ee\u9898\u662f\u6700\u5c0f\u5316\\(\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2\\)\u3002\u5bf9\u4e8e\u7ebf\u6027\u56de\u5f52\u8fd9\u6837\u7279\u6b8a\u7684\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u6c42\u51fa\u89e3\u6790\u89e3\uff08analytical solution\uff09\uff0c\u8bd5\u7740\u7528\u77e9\u9635\u6c42\u5bfc\u7684\u77e5\u8bc6\u6765\u6c42\u89e3\uff1a</p> <p>\u65b9\u4fbf\u8d77\u89c1\uff0c\u6211\u4eec\u5c06\u504f\u5dee\\(b\\)\u6dfb\u52a0\u5230\u77e9\u9635\\(\\mathbf{X}\\)\u4e2d\u3002\\(X \\leftarrow \\begin{bmatrix} X &amp; 1 \\\\\\end{bmatrix} \\quad w \\leftarrow \\begin{bmatrix} w \\\\ b \\end{bmatrix}\\)</p> \\[ \\ell(X,y,w) = \\frac{1}{2n} \\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\| ^2 \\] <p>\u7531\u77e9\u9635\u6c42\u5bfc\u6cd5\u5219\uff0c\u94fe\u5f0f\u6cd5\u5219\uff0c\u6211\u4eec\u6709\uff1a</p> \\[ \\begin{aligned} \\nabla_w \\ell(X,y,w) &amp;= \\frac{1}{n} \\nabla_w \\frac{1}{2} \\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\| ^2 \\\\ &amp;= \\frac{1}{n} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top \\mathbf{X}\\\\ \\end{aligned} \\] <p>\u4ee4\u4e0a\u5f0f\u4e3a0\uff0c\u6211\u4eec\u6709\uff1a</p> \\[ \\begin{aligned} \\frac{1}{n} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^\\top \\mathbf{X} &amp;= 0 \\\\ \\mathbf{X}^\\top (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) &amp;= 0 \\\\ \\mathbf{X}^\\top \\mathbf{X}\\mathbf{w} &amp;= \\mathbf{X}^\\top \\mathbf{y} \\\\ \\mathbf{w} &amp;= (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} \\\\ \\end{aligned} \\]"},{"location":"Notes/d2l/linear_network/#gradient-descent","title":"Gradient Descent \u68af\u5ea6\u4e0b\u964d","text":"<p>\u8fd9\u79cd\u65b9\u6cd5\u51e0\u4e4e\u53ef\u4ee5\u4f18\u5316\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u4e0d\u65ad\u5730\u5728\u635f\u5931\u51fd\u6570\u9012\u51cf\u7684\u65b9\u5411\u4e0a\u66f4\u65b0\u53c2\u6570\u6765\u964d\u4f4e\u8bef\u5dee\u3002</p> <p>\u68af\u5ea6\u4e0b\u964d\u6700\u7b80\u5355\u7684\u7528\u6cd5\u662f\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff08\u6570\u636e\u96c6\u4e2d\u6240\u6709\u6837\u672c\u7684\u635f\u5931\u5747\u503c\uff09 \u5173\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u5bfc\u6570\uff08\u5728\u8fd9\u91cc\u4e5f\u53ef\u4ee5\u79f0\u4e3a\u68af\u5ea6\uff09\u3002 \u4f46\u5b9e\u9645\u4e2d\u7684\u6267\u884c\u53ef\u80fd\u4f1a\u975e\u5e38\u6162\uff1a\u56e0\u4e3a\u5728\u6bcf\u4e00\u6b21\u66f4\u65b0\u53c2\u6570\u4e4b\u524d\uff0c\u6211\u4eec\u5fc5\u987b\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\u3002 \u56e0\u6b64\uff0c\u6211\u4eec\u901a\u5e38\u4f1a\u5728\u6bcf\u6b21\u9700\u8981\u8ba1\u7b97\u66f4\u65b0\u7684\u65f6\u5019\u968f\u673a\u62bd\u53d6\u4e00\u5c0f\u6279\u6837\u672c\uff0c \u8fd9\u79cd\u53d8\u4f53\u53eb\u505a\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08minibatch stochastic gradient descent\uff09\u3002</p> <p>\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u6211\u4eec\u9996\u5148\u968f\u673a\u62bd\u6837\u4e00\u4e2a\u5c0f\u6279\u91cf\\(\\mathcal{B}\\)\uff0c \u5b83\u662f\u7531\u56fa\u5b9a\u6570\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u7ec4\u6210\u7684\u3002 \u7136\u540e\uff0c\u6211\u4eec\u8ba1\u7b97\u5c0f\u6279\u91cf\u7684\u5e73\u5747\u635f\u5931\u5173\u4e8e\u6a21\u578b\u53c2\u6570\u7684\u5bfc\u6570\uff08\u4e5f\u53ef\u4ee5\u79f0\u4e3a\u68af\u5ea6\uff09\u3002 \u6700\u540e\uff0c\u6211\u4eec\u5c06\u68af\u5ea6\u4e58\u4ee5\u4e00\u4e2a\u9884\u5148\u786e\u5b9a\u7684\u6b63\u6570\\(\\eta\\) \uff0c\u5e76\u4ece\u5f53\u524d\u53c2\u6570\u7684\u503c\u4e2d\u51cf\u6389\u3002</p> \u8d85\u53c2\u6570 <p>\u8fd9\u4e2a\u9884\u5148\u786e\u5b9a\u7684\u6b63\u6570\\(\\eta\\)\u53eb\u505a\u5b66\u4e60\u7387\uff08learning rate \u7b80\u79f0lr\uff09\u6216\u6b65\u957f\uff08step size\uff09\u3002 \u4e0d\u80fd\u592a\u957f\u6216\u592a\u77ed\uff0c\u8c03\u6574\u8d85\u53c2\u6570\u7684\u8fc7\u7a0b\u5c31\u662f\u8c03\u53c2\u3002 \u5176\u4ed6\u8d85\u53c2\u6570\u8fd8\u6709\u6279\u91cf\u5927\u5c0f\uff08batch size\uff09\u548c\u8fed\u4ee3\u5468\u671f\u6570\uff08number of epochs\uff09\u3002</p> <p>\u6211\u4eec\u7528\u4e0b\u9762\u7684\u6570\u5b66\u516c\u5f0f\u6765\u8868\u793a\u8fd9\u4e00\u66f4\u65b0\u8fc7\u7a0b\uff08\\(\\partial\\)\u8868\u793a\u504f\u5bfc\u6570\uff09\uff1a</p> \\[(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).\\] \u52a8\u624b\u5199\u7ebf\u6027\u56de\u5f52 <pre><code>import torch\nimport random\n###################\n# step1 \u751f\u6210\u6570\u636e\u96c6 #\n###################\n\n# \u4f7f\u7528w=[2, -3.4], b=4.2\u4f5c\u4e3a\u771f\u5b9e\u53c2\u6570\uff0clen(w)=2 implies 2 features\ndef synthetic_data(w, b, num_examples): \n    \"\"\"\u751f\u6210 y = Xw + b + \u566a\u58f0\u3002\"\"\"\n    X = torch.normal(0, 1, (num_examples, len(w))) \n    y = torch.matmul(X, w) + b # y\u662f\u4e00\u4e2a\u884c\u5411\u91cf\n    y += torch.normal(0, 0.01, y.shape) # \u566a\u58f0\u9879\n    return X, y.reshape((-1, 1)) # \u5c06y\u8f6c\u6362\u4e3a\u5217\u5411\u91cf\n\ntrue_w = torch.tensor([2, -3.4])\ntrue_b = 4.2\nfeatures, labels = synthetic_data(true_w, true_b, 1000) #feature\u7684\u6bcf\u4e00\u884c\u662f\u4e00\u4e2a\u6837\u672c\n\uff0c\u542b\u6709\u4e24\u4e2afeature\n\n###################\n# step2 \u8bfb\u53d6\u6570\u636e\u96c6 #\n###################\n\ndef data_iter(batch_size, features, labels):\n    '''\u6267\u884cdata_iter(batch_size, features, labels)\u65f6\uff0c\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u751f\u6210\u5668\u3002\n    \u53ef\u4ee5\u901a\u8fc7\u5728\u8fd9\u4e2a\u751f\u6210\u5668\u4e0a\u8c03\u7528next()\u51fd\u6570\u6216\u8005\u4f7f\u7528for\u5faa\u73af\u6765\u83b7\u53d6\u6570\u636e\u3002\n    \u6bcf\u6b21\u83b7\u53d6\u6570\u636e\u65f6\uff0c\u5b83\u90fd\u4f1a\u8fd4\u56de\u4e00\u4e2a\u5305\u542bbatch_size\u4e2a\u6837\u672c\u7684\u6279\u6b21\uff0c\n    \u76f4\u5230\u6574\u4e2a\u6570\u636e\u96c6\u88ab\u904d\u5386\u5b8c\u3002'''\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices) # \u6253\u4e71\u7d22\u5f15\uff0c\u4f7f\u6837\u672c\u7684\u8bfb\u53d6\u987a\u5e8f\u662f\u968f\u673a\u7684\n    for i in range(0, num_examples, batch_size):\n        '''\u6700\u540e\u4e00\u6b21\u53ef\u80fd\u4e0d\u8db3\u4e00\u4e2abatch\uff0c\u6545\u53d6min(i + batch_size, num_examples)\n        # batch_indices\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3abatch_size(\u6709\u4e00\u4e2a\u53ef\u80fd\u7565\u5c0f)\u7684tensor\uff0c\u800c\u4e0d\u662flist\u8fd9\u662f\u4e3a\u4e86\u80fd\u652f\u6301GPU\u64cd\u4f5c'''\n        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])\n        yield features[batch_indices], labels[batch_indices]\n\nbatch_size = 10 #\u6bcf\u4e2abatch\u542b\u670910\u4e2a\u6837\u672c\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break\n\n#################\n# step3 \u5efa\u7acb\u6a21\u578b #\n#################\n\nw = torch.normal(0, 0.01, size=(2,1), requires_grad=True) # \u9884\u7559\u68af\u5ea6\nb = torch.zeros(1, requires_grad=True)\n\ndef linreg(X, w, b): \n    \"\"\"\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u3002\"\"\"\n    return torch.matmul(X, w) + b\n\ndef squared_loss(y_hat, y): \n    \"\"\"\u5747\u65b9\u635f\u5931\u3002\"\"\"\n    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n\ndef sgd(params, lr, batch_size): \n    \"\"\"\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u3002sgd stand for stochastic(random) gradient descent, \n    params is a list of parameters that need to be updated in the model, \n    specifically, params = [w, b]\"\"\"\n    with torch.no_grad(): #\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPyTorch\u4f1a\u4e3a\u8ddf\u8e2a\u5176\u68af\u5ea6\u7684\u6bcf\u4e2a\u5f20\u91cf\u542f\u7528\u68af\u5ea6\u8ddf\u8e2a\u3002\n        for param in params:\n            param -= lr * param.grad / batch_size\n            param.grad.zero_()\n\n##############\n# step4 \u8bad\u7ec3 #\n##############\n\n#\u8bbe\u7f6e\u8d85\u53c2\u6570\nlr = 0.03\nnum_epochs = 3\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y)  # X\u548cy\u7684\u5c0f\u6279\u91cf\u635f\u5931\n        # \u56e0\u4e3al\u5f62\u72b6\u662f(batch_size,1)\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u6807\u91cf\u3002l\u4e2d\u7684\u6240\u6709\u5143\u7d20\u88ab\u52a0\u5230\u4e00\u8d77\uff0c\n        # \u5e76\u4ee5\u6b64\u8ba1\u7b97\u5173\u4e8e[w,b]\u7684\u68af\u5ea6\n        l.sum().backward()\n        sgd([w, b], lr, batch_size)  # \u4f7f\u7528\u53c2\u6570\u7684\u68af\u5ea6\u66f4\u65b0\u53c2\u6570\n    with torch.no_grad():\n        train_l = loss(net(features, w, b), labels)\n        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n</code></pre>"},{"location":"Notes/d2l/linear_network/#softmax","title":"Softmax\u56de\u5f52","text":"<p>\u56de\u5f52\u53ef\u4ee5\u9884\u6d4b\u201c\u591a\u5c11\u201d\u7684\u95ee\u9898\uff0c\u800c\u5206\u7c7b\u5219\u53ef\u4ee5\u9884\u6d4b\u201c\u54ea\u4e00\u4e2a\u201d\u7684\u95ee\u9898\u3002Softmax\u56de\u5f52\u662f\u4e00\u4e2a\u5355\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u591a\u5206\u7c7b\u95ee\u9898\u3002\u6211\u4eec\u4ecd\u7136\u4f7f\u7528\u7ebf\u6027\u4ee3\u6570\u7b26\u53f7\u3002 \u901a\u8fc7\u5411\u91cf\u5f62\u5f0f\u8868\u8fbe\u4e3a\\(\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\\)\uff0c \u8fd9\u662f\u4e00\u79cd\u66f4\u9002\u5408\u6570\u5b66\u548c\u7f16\u5199\u4ee3\u7801\u7684\u5f62\u5f0f\u3002 \u5bf9\u4e8e\u7ed9\u5b9a\u6570\u636e\u6837\u672c\u7684\u7279\u5f81\\(\\mathbf{x}\\)\uff0c \u6211\u4eec\u7684\u8f93\u51fa\u662f\u7531\u6743\u91cd\u4e0e\u8f93\u5165\u7279\u5f81\u8fdb\u884c\u77e9\u9635-\u5411\u91cf\u4e58\u6cd5\u518d\u52a0\u4e0a\u504f\u7f6e\\(\\mathbf{b}\\)\u5f97\u5230\u7684\u3002</p> \u4e00\u4e9b\u6982\u5ff5 <ul> <li>one-hot code\uff08\u72ec\u70ed\u7f16\u7801\uff09\uff1a\u5411\u91cf\u7684\u957f\u5ea6\u662f\u7c7b\u522b\u7684\u4e2a\u6570\uff0c\u5411\u91cf\u7684\u7b2c\\(i\\)\u4e2a\u5143\u7d20\u4e3a1\uff0c\u5176\u4f59\u5143\u7d20\u4e3a0\uff0c\u7b2c\\(i\\)\u4e2a\u5143\u7d20\u5bf9\u5e94\u7684\u7c7b\u522b\u5373\u4e3a\u8be5\u6570\u5b57\u6240\u4ee3\u8868\u7684\u7c7b\u522b\u3002\u6bd4\u5982\u732b\u72d7\u5206\u7c7b\u95ee\u9898\uff0c\u732b\u7684one-hot code\u4e3a[1,0]\uff0c\u72d7\u7684one-hot code\u4e3a[0,1]\u3002\u5219\u6807\u7b7e\u503c\\(y \\in \\{ [1,0], [0,1] \\}\\)</li> <li>\u5168\u8fde\u63a5\u5c42\uff1a\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165\u662f\u4e00\u4e2a\u5411\u91cf(\u6bd4\u5982\u4e00\u4e2a\u6837\u672c\u7684\u6240\u6709\u7279\u5f81)\uff0c\u8f93\u51fa\u4e5f\u662f\u4e00\u4e2a\u5411\u91cf \uff08\u6bd4\u5982\u6240\u6709\u7c7b\u522b\u7684\u9884\u6d4b\u503c\uff0c\u6ce8\u610f\u6b64\u65f6\u8fd8\u4e0d\u662f\u89c4\u8303\u540e\u7684\u6982\u7387\uff09\u3002\u5168\u8fde\u63a5\u5c42\u7684\u6bcf\u4e2a\u8f93\u51fa\u5143\u7d20\u90fd\u4e0e\u8f93\u5165\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u76f8\u8fde\uff0c\u4e14\u6bcf\u4e2a\u8fde\u63a5\u90fd\u6709\u6743\u91cd\u3002</li> </ul>"},{"location":"Notes/d2l/linear_network/#softmax_1","title":"softmax\u8fd0\u7b97","text":"<p>\u6211\u4eec\u5e0c\u671b\u6a21\u578b\u7684\u8f93\u51fa\\(\\hat{y}_j\\)\u53ef\u4ee5\u89c6\u4e3a\u5c5e\u4e8e\u7c7b\\(j\\)\u7684\u6982\u7387\uff0c \u7136\u540e\u9009\u62e9\u5177\u6709\u6700\u5927\u8f93\u51fa\u503c\u7684\u7c7b\u522b\\(\\operatorname*{argmax}_j y_j\\)\u4f5c\u4e3a\u6211\u4eec\u7684\u9884\u6d4b\u3002 \u4f8b\u5982\uff0c\u5982\u679c\\(\\hat{y}_1\\)\u3001\\(\\hat{y}_2\\)\u548c\\(\\hat{y}_3\\)\u5206\u522b\u4e3a0.1\u30010.8\u548c0.1\uff0c \u90a3\u4e48\u6211\u4eec\u9884\u6d4b\u7684\u7c7b\u522b\u662f2</p> <p>\u901a\u8fc7\\(\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\\)\uff0c\u8fd0\u7b97\u5f97\u5230\u7684\\(\\mathbf{o}\\)\u4e0d\u4e00\u5b9a\u662f\u6982\u7387\uff0c\u56e0\u4e3a\u53ef\u80fd\u6709\u8d1f\u6570\u6216\u8005\u5927\u4e8e1\u7684\u6570\u3002\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5bf9\\(\\mathbf{o}\\)\u8fdb\u884c\u89c4\u8303\u5316\uff0c\u4f7f\u5176\u6bcf\u4e2a\u5143\u7d20\u90fd\u57280\u548c1\u4e4b\u95f4\u4e14\u6240\u6709\u5143\u7d20\u4e4b\u548c\u4e3a1\u3002\u8fd9\u6837\u6211\u4eec\u7684\u8f93\u51fa\u5c31\u53ef\u4ee5\u770b\u4f5c\u6982\u7387\u4e86\u3002\uff0c\u4e00\u4e2a\u6210\u719f\u7684\u65b9\u6cd5\u662f\u901a\u8fc7softmax\u8fd0\u7b97\u6765\u83b7\u5f97\u4e00\u4e2a\u6709\u6548\u7684\u7c7b\u522b\u6982\u7387\u5206\u5e03\u3002</p> \\[ \\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o})\\quad \\text{\u5176\u4e2d}\\quad \\hat{y}_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)} \\] <p>\u5b83\u6709\u5f88\u597d\u7684\u6027\u8d28\uff0c\u5f53\u6211\u4eec\u4f7f\u7528\u5982\u4e0b\u5f0f\u5b50\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u65f6\uff0c\u5c31\u53ef\u4ee5\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u4f3c\u7136\u3002(q\u4e3a\u7c7b\u522b\u6570)\u5b83\u88ab\u79f0\u4e3a\u4ea4\u53c9\u71b5\u635f\u5931\uff08cross-entropy loss\uff09\u3002</p> \\[ l(\\mathbf{y}, \\hat{\\mathbf{y}})= - \\sum_{j}^{q} y_j \\log \\hat{y}_j \\] <p>\u53ef\u4ee5\u8fd9\u4e48\u7406\u89e3\uff1a\u8003\u8651\u6781\u9650\u60c5\u51b5\uff0c\u5f53\\(\\hat{y}_j=1\\)\u65f6\uff0c\\(l(\\mathbf{y}, \\hat{\\mathbf{y}})=0\\)\uff0c\u5f53\\(\\hat{y}_j=0\\)\u65f6\uff0c\\(l(\\mathbf{y}, \\hat{\\mathbf{y}})=\\infty\\)\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5e0c\u671b\\(\\hat{y}_j\\)\u7b49\u4e8e\u771f\u5b9e\u6807\u7b7e\\(y_j\\)\uff0c\u5373\\(\\hat{y}_j=y_j\\)\u3002 </p> <p>\u66f4\u4e25\u8c28\u5730\uff0c\u6211\u4eec\u53ef\u4ee5\u8bc1\u660e\u6700\u5927\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u4f3c\u7136\u7b49\u4ef7\u4e8e\u6700\u5c0f\u5316\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\uff0c\u5373\\(l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j}^{q} y_j \\log \\hat{y}_j\\)\u3002\uff08\u4ee5\u4e0b\\(y_j\uff0co_j\\)\u5747\u4e3a\u5b9e\u6570\uff0c\u4e14\\(y_j\uff0cj=1, 2, \\cdots, q\\) \u6709\u4e14\u4ec5\u6709\u4e00\u4e2a\u6807\u7b7e\u503c\u4e3a1\uff0c\u5176\u4f59\u5747\u4e3a0\uff09</p> \\[ \\begin{aligned} l(\\mathbf{y}, \\hat{\\mathbf{y}}) &amp;=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\ &amp;= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j\\\\ &amp;= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j. \\end{aligned} \\] \\[ \\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j. \\] <p>\u89c2\u5bdf\u4ee5\u4e0a\u5f0f\u5b50\uff0c\u53d1\u73b0\u5bfc\u6570\u662fsoftmax\u6a21\u578b\u5206\u914d\u7684\u6982\u7387\u4e0e\u5b9e\u9645\u53d1\u751f\u7684\u60c5\u51b5\uff08\u7531\u72ec\u70ed\u6807\u7b7e\u5411\u91cf\u8868\u793a\uff09\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u4ece\u8fd9\u4e2a\u610f\u4e49\u4e0a\u8bb2\uff0c\u8fd9\u4e0e\u6211\u4eec\u5728\u56de\u5f52\u4e2d\u770b\u5230\u7684\u975e\u5e38\u76f8\u4f3c\uff0c\u5176\u4e2d\u68af\u5ea6\u662f\u89c2\u6d4b\u503c\\(y\\)\u548c\u4f30\u8ba1\u503c\\(\\hat{y}\\)\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u8fd9\u5c31\u662f\u9009\u53d6softmax\u6765\u6807\u51c6\u5316\u8f93\u51fa\u7684\u539f\u56e0\u3002</p> \u52a8\u624b\u5199softmax\u56de\u5f52 <pre><code>import torch\nimport torchvision\nfrom torch.utils import data\nfrom torchvision import transforms\nd2l.use_svg_display()\n</code></pre>"},{"location":"Notes/d2l/preliminaries/","title":"Preliminaries","text":""},{"location":"Notes/d2l/preliminaries/#calculus","title":"Calculus","text":""},{"location":"Notes/d2l/preliminaries/#gradient","title":"Gradient \u68af\u5ea6","text":"<p>\u6211\u4eec\u53ef\u4ee5\u8fde\u7ed3\u4e00\u4e2a\u591a\u5143\u51fd\u6570\u5bf9\u5176\u6240\u6709\u53d8\u91cf\u7684\u504f\u5bfc\u6570\uff0c\u4ee5\u5f97\u5230\u8be5\u51fd\u6570\u7684\u68af\u5ea6\uff08gradient\uff09\u5411\u91cf\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8bbe\u51fd\u6570\\(f:\\mathbb{R}^n\\rightarrow\\mathbb{R}\\)\u7684\u8f93\u5165\u662f \u4e00\u4e2a\\(n\\)\u7ef4\u5411\u91cf\\(\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top\\)\uff0c\u5e76\u4e14\u8f93\u51fa\u662f\u4e00\u4e2a\u6807\u91cf\u3002 \u51fd\u6570\\(f(\\mathbf{x})\\)\u76f8\u5bf9\u4e8e\\(\\mathbf{x}\\)\u7684\u68af\u5ea6\u662f\u4e00\u4e2a\u5305\u542b\\(n\\)\u4e2a\u504f\u5bfc\u6570\u7684\u5411\u91cf:</p> \\[ \\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg], \\] Example <p>\u8bc1\u660e\u5bf9\u4e8e\u6240\u6709\\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\)\uff0c\u90fd\u6709\\(\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}\\)</p> <p>\u4ee4\uff1a</p> \\[ A= \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{n2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}_{m \\times n} \\] <p>\u4ee4\uff1a</p> \\[ b_i=\\begin{bmatrix} a_{i_{1i}}, a_{2i}, \\cdots, a_{mi} \\end{bmatrix}^T \\] <p>\u4e8e\u662f\uff1a</p> \\[ f(x)=\\sum_{i=1}^n b_i x_i \\] <p>\u6309\u7167\u5b9a\u4e49\uff1a</p> \\[ \\nabla_{\\mathbf{x}}f(x)=\\begin{bmatrix} b_1, b_2, \\cdots, b_n \\end{bmatrix}^T=A \\] <p>\u8bc1\u6bd5\u3002</p> <p>\u8fd8\u6709\u5982\u4e0b\u5e38\u89c1\u7684\u6027\u8d28\uff1a(\\(x \\in \\mathbb{R}^{n\\times 1}\\)\uff0c\u4e4b\u540e\u5747\u7b80\u5199\u4e3a\\(\\mathbb{R}^n\\))</p> <ul> <li>\u5bf9\u4e8e\u6240\u6709\\(\\mathbf{A} \\in \\mathbb{R}^{n \\times m}\\)\uff0c\u90fd\u6709\\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}^\\top\\)</li> <li>\u5bf9\u4e8e\u6240\u6709\\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\)\uff0c\u90fd\u6709\\(\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}\\)</li> <li>\\(\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}^\\top\\)</li> </ul>"},{"location":"Notes/d2l/preliminaries/#autograd-pytorch","title":"Autograd Pytorch\u81ea\u52a8\u6c42\u5bfc","text":"<p>\u5047\u8bbe\u6211\u4eec\u60f3\u5bf9\u51fd\u6570\\(y=2\\mathbf{x}^{\\top}\\mathbf{x}\\)\u5173\u4e8e\u5217\u5411\u91cf\\(\\mathbf{x}\\)\u6c42\u5bfc\u3002</p> <p>\u9996\u5148\uff0c\u6211\u4eec\u521b\u5efa\u53d8\u91cf<code>x</code>\u5e76\u4e3a\u5176\u5206\u914d\u4e00\u4e2a\u521d\u59cb\u503c\u3002 </p><pre><code>import torch\n\nx = torch.arange(4.0)\nx\n</code></pre> tensor([0., 1., 2., 3.])  <p>\u5728\u6211\u4eec\u8ba1\u7b97\\(y\\)\u5173\u4e8e\\(\\mathbf{x}\\)\u7684\u68af\u5ea6\u4e4b\u524d\uff0c\u9700\u8981\u4e00\u4e2a\u5730\u65b9\u6765\u5b58\u50a8\u68af\u5ea6\u3002 \u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u4e0d\u4f1a\u5728\u6bcf\u6b21\u5bf9\u4e00\u4e2a\u53c2\u6570\u6c42\u5bfc\u65f6\u90fd\u5206\u914d\u65b0\u7684\u5185\u5b58\u3002 \u56e0\u4e3a\u6211\u4eec\u7ecf\u5e38\u4f1a\u6210\u5343\u4e0a\u4e07\u6b21\u5730\u66f4\u65b0\u76f8\u540c\u7684\u53c2\u6570\uff0c\u6bcf\u6b21\u90fd\u5206\u914d\u65b0\u7684\u5185\u5b58\u53ef\u80fd\u5f88\u5feb\u5c31\u4f1a\u5c06\u5185\u5b58\u8017\u5c3d\u3002 \u6ce8\u610f\uff0c\u4e00\u4e2a\u6807\u91cf\u51fd\u6570\u5173\u4e8e\u5411\u91cf\\(\\mathbf{x}\\)\u7684\u68af\u5ea6\u662f\u5411\u91cf\uff0c\u5e76\u4e14\u4e0e\\(\\mathbf{x}\\)\u5177\u6709\u76f8\u540c\u7684\u5f62\u72b6\u3002</p> <pre><code>x.requires_grad_(True)  # \u7b49\u4ef7\u4e8ex=torch.arange(4.0,requires_grad=True)\nx.grad  # \u9ed8\u8ba4\u503c\u662fNone\n</code></pre> <p>\u73b0\u5728\u8ba9\u6211\u4eec\u8ba1\u7b97\\(y\\)\u3002 </p><pre><code>y = 2 * torch.dot(x, x)\ny\n</code></pre> <p>tensor(28., grad_fn=&lt;MulBackward0&gt;)</p> <p><code>x</code>\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a4\u7684\u5411\u91cf\uff0c\u8ba1\u7b97<code>x</code>\u548c<code>x</code>\u7684\u70b9\u79ef\uff0c\u5f97\u5230\u4e86\u6211\u4eec\u8d4b\u503c\u7ed9<code>y</code>\u7684\u6807\u91cf\u8f93\u51fa\u3002 \u63a5\u4e0b\u6765\uff0c\u901a\u8fc7\u8c03\u7528\u53cd\u5411\u4f20\u64ad\u51fd\u6570\u6765\u81ea\u52a8\u8ba1\u7b97<code>y</code>\u5173\u4e8e<code>x</code>\u6bcf\u4e2a\u5206\u91cf\u7684\u68af\u5ea6\uff0c\u5e76\u6253\u5370\u8fd9\u4e9b\u68af\u5ea6\u3002</p> <pre><code>y.backward()\nx.grad\n</code></pre> <p>tensor([ 0.,  4.,  8., 12.])</p> <p>\u663e\u7136\u5c31\u662f4\u500d\u7684<code>x</code>\uff0c\u8ba1\u7b97\u6b63\u786e\u3002</p>"},{"location":"Notes/d2l/preliminaries/#_1","title":"\u975e\u6807\u91cf\u5173\u4e8e\u5411\u91cf\u7684\u68af\u5ea6","text":"<p>\u3002\u5f53<code>y</code>\u4e0d\u662f\u6807\u91cf\u65f6\uff0c\u5411\u91cf<code>y</code>\u5173\u4e8e\u5411\u91cf<code>x</code>\u7684\u5bfc\u6570\u7684\u6700\u81ea\u7136\u89e3\u91ca\u662f\u4e00\u4e2a\u77e9\u9635\u3002\u5bf9\u4e8e\u9ad8\u9636\u548c\u9ad8\u7ef4\u7684<code>y</code>\u548c<code>x</code>\uff0c\u6c42\u5bfc\u7684\u7ed3\u679c\u53ef\u4ee5\u662f\u4e00\u4e2a\u9ad8\u9636\u5f20\u91cf\u3002</p> <p>\u7136\u800c\uff0c\u867d\u7136\u8fd9\u4e9b\u66f4\u5947\u7279\u7684\u5bf9\u8c61\u786e\u5b9e\u51fa\u73b0\u5728\u9ad8\u7ea7\u673a\u5668\u5b66\u4e60\u4e2d\uff08\u5305\u62ec[\u6df1\u5ea6\u5b66\u4e60\u4e2d]\uff09\uff0c \u4f46\u5f53\u8c03\u7528\u5411\u91cf\u7684\u53cd\u5411\u8ba1\u7b97\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u4f1a\u8bd5\u56fe\u8ba1\u7b97\u4e00\u6279\u8bad\u7ec3\u6837\u672c\u4e2d\u6bcf\u4e2a\u7ec4\u6210\u90e8\u5206\u7684\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u3002 \u8fd9\u91cc\u6211\u4eec\u7684\u76ee\u7684\u4e0d\u662f\u8ba1\u7b97\u5fae\u5206\u77e9\u9635\uff0c\u800c\u662f\u5355\u72ec\u8ba1\u7b97\u6279\u91cf\u4e2d\u6bcf\u4e2a\u6837\u672c\u7684\u504f\u5bfc\u6570\u4e4b\u548c\u3002</p> <p>\u5bf9\u975e\u6807\u91cf\u8c03\u7528<code>.backward()</code>\u9700\u8981\u4f20\u5165\u4e00\u4e2a<code>gradient</code>\u53c2\u6570\u3002\u56e0\u4e3a\u5f53<code>y</code>\u4e0d\u662f\u6807\u91cf\u65f6\uff0c\u4e0d\u80fd\u76f4\u63a5\u8ba1\u7b97<code>y</code>\u5173\u4e8e<code>x</code>\u7684\u68af\u5ea6\uff0c\u56e0\u4e3a\u68af\u5ea6\u662f\u6807\u91cf\u5bf9\u5411\u91cf\u7684\u5bfc\u6570\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5b9e\u9645\u4e0a\u8ba1\u7b97\u7684\u662f\u5411\u91cf<code>y</code>\u548c<code>gradient</code>\u53c2\u6570\u7684\u70b9\u79ef\u5173\u4e8e<code>x</code>\u7684\u68af\u5ea6\uff0c\u5b83\u6307\u5b9a\u4e86y\u7684\u6bcf\u4e2a\u5206\u91cf\u5bf9x\u7684\u68af\u5ea6\u7684\u6743\u91cd\u3002 </p><pre><code>x.grad.zero_()# \u68af\u5ea6\u6e05\u96f6\uff0c\u5426\u5219\u4f1a\u7d2f\u52a0\ny = x * x # \u6ce8\u610f*\u662f\u6309\u5143\u7d20\u76f8\u4e58\ny.sum().backward()# \u7b49\u4ef7\u4e8ey.backward(torch.ones(len(x)))\nx.grad\n</code></pre> <p>tensor([0., 2., 4., 6.])</p> <p>\u6c42\u5bfc\u5e94\u4e3a2x\uff0c\u8ba1\u7b97\u6b63\u786e\u3002</p>"},{"location":"Notes/d2l/preliminaries/#_2","title":"\u5206\u79bb\u8ba1\u7b97","text":"<p>\u6709\u65f6\uff0c\u6211\u4eec\u5e0c\u671b\u5c06\u67d0\u4e9b\u8ba1\u7b97\u79fb\u52a8\u5230\u8bb0\u5f55\u7684\u8ba1\u7b97\u56fe\u4e4b\u5916\u3002 \u4f8b\u5982\uff0c\u5047\u8bbe<code>y</code>\u662f\u4f5c\u4e3a<code>x</code>\u7684\u51fd\u6570\u8ba1\u7b97\u7684\uff0c\u800c<code>z</code>\u5219\u662f\u4f5c\u4e3a<code>y</code>\u548c<code>x</code>\u7684\u51fd\u6570\u8ba1\u7b97\u7684\u3002 \u73b0\u5728\uff0c\u60f3\u8c61\u4e00\u4e0b\uff0c\u6211\u4eec\u60f3\u8ba1\u7b97<code>z</code>\u5173\u4e8e<code>x</code>\u7684\u68af\u5ea6\uff0c\u4f46\u7531\u4e8e\u67d0\u79cd\u539f\u56e0\uff0c\u6211\u4eec\u5e0c\u671b\u5c06<code>y</code>\u89c6\u4e3a\u4e00\u4e2a\u5e38\u6570\uff0c\u5e76\u4e14\u53ea\u8003\u8651\u5230<code>x</code>\u5728<code>y</code>\u88ab\u8ba1\u7b97\u540e\u53d1\u6325\u7684\u4f5c\u7528\u3002</p> <p>\u8fd9\u91cc\u53ef\u4ee5\u5206\u79bb<code>y</code>\u6765\u8fd4\u56de\u4e00\u4e2a\u65b0\u53d8\u91cf<code>u</code>\uff0c\u8be5\u53d8\u91cf\u4e0e<code>y</code>\u5177\u6709\u76f8\u540c\u7684\u503c\uff0c \u4f46\u4e22\u5f03\u8ba1\u7b97\u56fe\u4e2d\u5982\u4f55\u8ba1\u7b97<code>y</code>\u7684\u4efb\u4f55\u4fe1\u606f\u3002 \u6362\u53e5\u8bdd\u8bf4\uff0c\u68af\u5ea6\u4e0d\u4f1a\u5411\u540e\u6d41\u7ecf<code>u</code>\u5230<code>x</code>\u3002 \u56e0\u6b64\uff0c\u4e0b\u9762\u7684\u53cd\u5411\u4f20\u64ad\u51fd\u6570\u8ba1\u7b97<code>z=u*x</code>\u5173\u4e8e<code>x</code>\u7684\u504f\u5bfc\u6570\uff0c\u540c\u65f6\u5c06<code>u</code>\u4f5c\u4e3a\u5e38\u6570\u5904\u7406\uff0c \u800c\u4e0d\u662f<code>z=x*x*x</code>\u5173\u4e8e<code>x</code>\u7684\u504f\u5bfc\u6570\u3002 </p><pre><code>x.grad.zero_()\ny = x * x\nu = y.detach()\nz = u * x  #z\u662f\u4e00\u4e2a\u5411\u91cf\n\nz.sum().backward()\nx.grad\n</code></pre> <p>tensor([0., 1., 4., 9.]) \u6c42\u5bfc\u4e3au\uff0c\u8ba1\u7b97\u6b63\u786e\u3002\u5176\u4e2d\uff0c\\(u=\\begin{bmatrix} 0 &amp; 1 &amp; 4 &amp; 9 \\\\\\end{bmatrix}\\)</p>"},{"location":"Notes/music%20composition/better%20bass/","title":"Ep10 Ep17 Better Bass","text":""},{"location":"Notes/music%20composition/better%20bass/#ep10-ep17-better-bass","title":"Ep10 Ep17 Better Bass","text":"<ol> <li> <p>Walking Bass\uff1a</p> <ol> <li>\u6bcf\u4e00\u62cd\u4e00\u4e2a\u97f3\uff0c\u6bcf\u4e2a\u548c\u5f26\u7684\u7b2c\u4e00\u62cd\u5f39\u6839\u97f3\uff0c\u5176\u4ed6\u62cd\u5f39\u548c\u5f26\u5185\u97f3\u3002</li> <li>\u6362\u548c\u5f26\u65f6\uff0c\u5c3d\u91cf\u8d70\u5230\u76ee\u6807\u97f3\uff0c\u5373\u4e0a\u4e00\u548c\u5f26\u6700\u540e\u4e00\u4e2a\u97f3\u4e0e\u4e0b\u4e00\u548c\u5f26\u7b2c\u4e00\u4e2a\u97f3\u7684\u97f3\u7a0b\u63a5\u8fd1\u3002</li> <li>\u76f4\u63a5\u7528\u4e0b\u4e00\u4e2a\u548c\u5f26\u6839\u97f3\u4f4e\u534a\u97f3\u4f5c\u4e3a\u73b0\u5728\u5c0f\u8282\u7684\u6700\u540e\u4e00\u62cd\u3002</li> </ol> </li> <li> <p>\u52a8\u611f\u4e00\u4e9b\u7684Bass</p> <ol> <li>\u9664\u4e86\u6839\u97f3\u4ee5\u5916\uff0c\u53ef\u4ee5\u52a0\u51655\u97f3\uff0c7\u97f3\uff0c13\u97f3\u548c\u4e00\u4e9b\u7ecf\u8fc7\u97f3\u3002</li> <li>\u5f3a\u5316\u4e3b\u8981\u7684\u97f3\uff0c\u5f31\u5316\u7ecf\u8fc7\u97f3\uff0c\u88c5\u9970\u97f3\uff0c\u8282\u594f\u97f3\u3002</li> </ol> </li> </ol>"},{"location":"Notes/music%20composition/ep28-30%E5%89%AF%E5%B1%9E%E5%92%8C%E5%BC%A6/","title":"ep28-30 \u526f\u5c5e\u548c\u5f26","text":""},{"location":"Notes/music%20composition/ep28-30%E5%89%AF%E5%B1%9E%E5%92%8C%E5%BC%A6/#ep28-30","title":"ep28-30 \u526f\u5c5e\u548c\u5f26","text":"<ol> <li>\u539f\u7406\uff1a\u5c5e\u4e03\u548c\u5f26\u60f3\u8981\u89e3\u51b3\u5230\u4e3b\u548c\u5f26\uff0c\u5982\u679c\u628a\u76ee\u6807\u548c\u5f26\u5f53\u4f5c\u6682\u65f6\u7684\u201c\u5bb6\u201d\uff0c\u90a3\u4e48\u5728\u4e14\u6362\u548c\u5f26\u4e2d\u95f4\u63d2\u5165\u4e00\u4e2a\u526f\u5c5e\u548c\u5f26\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e0d\u9519\u7684\u6548\u679c\u3002\\(C\\to Em\\)\u5c31\u53ef\u4ee5\u6362\u6210\\(C \\to B_{7} \\to Em\\)\uff0c\u5176\u4e2d\\(B_{7}\\)\u662f\\(Em\\)\u5c0f\u8c03\u7684\u5c5e\u4e03\u548c\u5f26\u3002    </li> <li>\uff08\u8fdb\u9636\u614e\u7528\uff09\u964d2\u4ee35\uff1a\u7528\u9ad8\u534a\u97f3\u7684\u5c5e\u4e03/\u4e5d/\u5341\u4e00\u548c\u5f26\u4ee3\u66ff\u76ee\u6807\u548c\u5f26\u7684\u5c5e\u4e03\u548c\u5f26\uff0c\u4f8b\u5982\\(C\\to B_{7} \\to Em\\)\u53ef\u4ee5\u6362\u6210\\(C \\to F^b_{7} \\to Em\\)\u3002    </li> <li>\u63a51\uff0c\u5c06\u76ee\u6807\u548c\u5f26\u5f53\u4f5c\u201c\u5bb6\u201d\uff0c\u5728\u76ee\u6807\u548c\u5f26\u4e4b\u524d\u63d2\u5165\u4e00\u4e2a\u526f\u5c5e\u548c\u5f26\uff08V\u7ea7\uff09\u518d\u5728\u526f\u5c5e\u548c\u5f26\u4e4b\u524d\u63d2\u5165\u4e00\u4e2a\u201c\u6865\u201d\uff08II\u7ea7\uff09\uff0c\u8fd9\u6837\u542c\u611f\u66f4\u6d41\u884c\u3002\\(C \\to F^{maj_{7}}\\)\u5c31\u53ef\u4ee5\u6362\u6210\\(C \\to Gm^7 \\to C^7 \\to F^{maj7}\\)     \u6ce8\u610f\uff0c\u5f53\u76ee\u6807\u548c\u5f26\u662f\u5c0f\u548c\u5f26\u65f6\uff0c\u5728\u524d\u9762\u88c5\u9970\u7684\u4e8c\u7ea7\u548c\u5f26\u9700\u8981\u53d8\u6210\u4e00\u4e2a\u534a\u51cf\u4e03\u548c\u5f26\uff08\u5c0f\u4e09\u5ea6+\u5c0f\u4e09\u5ea6+\u5927\u4e09\u5ea6\uff09    eg\uff1a\\(C \\to Am\\)\u53d8\u6210\\(C \\to  B^{\\varnothing}_7 \\to E_{7} \\to Am\\)    \u5f88\u6709\u6d41\u884c\u7684\u5473\u9053!</li> </ol>"},{"location":"Notes/music%20composition/ep31.%E5%92%8C%E5%BC%A6%E5%88%86%E8%A7%A3/","title":"ep31.\u548c\u5f26\u5206\u89e3","text":""},{"location":"Notes/music%20composition/ep31.%E5%92%8C%E5%BC%A6%E5%88%86%E8%A7%A3/#ep31","title":"ep31.\u548c\u5f26\u5206\u89e3","text":"<ol> <li>\u6362\u548c\u5f26\u7684\u7b2c\u4e00\u62cd\u5f39\u4f4e\u97f3\uff0c\u7136\u540e\u91cd\u590d\u548c\u5f26\uff0c\u7c7b\u4f3c\u4e8e\u201c\u8e66\u6070\u6070\u6070\u201d \u201c\u8e66\u6070\u8e66\u6070\u201d</li> <li>\u516b\u5206\u97f3\u7b26\u67f1\u5f0f\uff0c\u57281\u30014\u30017\u5904\u5f39\u4f4e\u97f3</li> <li>\u5148\u5206\u89e3\uff0c\u518d\u548c\u5f26\uff0c\u4e0d\u4e00\u5b9a\u4e25\u683c\u5411\u4e0a\u722c\u884c</li> </ol>"},{"location":"Notes/music%20composition/%E4%B8%8D%E9%94%99%E7%9A%84%E9%9F%B3%E8%89%B2/","title":"\u4e0d\u9519\u7684\u97f3\u8272","text":""},{"location":"Notes/music%20composition/%E4%B8%8D%E9%94%99%E7%9A%84%E9%9F%B3%E8%89%B2/#_1","title":"\u4e0d\u9519\u7684\u97f3\u8272","text":""},{"location":"Notes/music%20composition/%E4%B8%8D%E9%94%99%E7%9A%84%E9%9F%B3%E8%89%B2/#_2","title":"\u97f3\u6548","text":"<ol> <li>Gravity - aggressive - bending destruction  \u7d27\u5f20</li> <li>Gravity - ethereal - the beehive  \u7a7a\u7075</li> </ol>"},{"location":"Notes/music%20composition/%E9%9F%B3%E7%A8%8B/","title":"\u97f3\u7a0b","text":""},{"location":"Notes/music%20composition/%E9%9F%B3%E7%A8%8B/#_1","title":"\u97f3\u7a0b","text":""},{"location":"Notes/music%20composition/%E9%9F%B3%E7%A8%8B/#_2","title":"\u8ba1\u7b97\uff08\u4e24\u97f3\u76f8\u51cf\uff09","text":"<ol> <li>\u5c0f\u4e8c\u5ea6</li> <li>\u5927\u4e8c\u5ea6</li> <li>\u5c0f\u4e09\u5ea6</li> <li>\u5927\u4e09\u5ea6</li> <li>\u7eaf\u56db\u5ea6</li> <li>\u589e\u56db\u5ea6/\u51cf\u4e94\u5ea6</li> <li>\u7eaf\u4e94\u5ea6</li> <li>\u5c0f\u516d\u5ea6/\u589e\u4e94\u5ea6</li> <li>\u5927\u516d\u5ea6/\u51cf\u4e03\u5ea6</li> <li>\u5c0f\u4e03\u5ea6</li> <li>\u5927\u4e03\u5ea6</li> <li>\u7eaf\u516b\u5ea6</li> </ol> Example <p>\u4e24\u7ec4\u97f3\u7a0b\u76f8\u52a0\u4e3a14\u65f6\uff0c\u4e24\u7ec4\u97f3\u7a0b\u4e92\u4e3a\u4e92\u8865\u97f3\u7a0b\uff0c\u5177\u6709\uff1a</p> <ol> <li>\u589e\u5bf9\u5e94\u51cf\uff0c\u5c0f\u5bf9\u5e94\u5927\uff0c\u7eaf\u5bf9\u5e94\u7eaf</li> <li>\u4e24\u97f3\u7a0b\u5ea6\u6570\u76f8\u52a0\u4e3a9 </li> </ol>"},{"location":"Notes/music%20composition/%E9%9F%B3%E7%A8%8B/#_3","title":"\u548c\u5f26\u7684\u6784\u6210\uff08\u97f3\u7a0b\u5747\u4e0e\u6839\u97f3\u6bd4\u8f83\uff09","text":"<ol> <li>\u5927\u4e09\u548c\u5f26\uff1a\u5927\u4e09\u5ea6+\u7eaf\u4e94\u5ea6\uff08\u4f8b\u5982C: C E G; E-C=4, G-C=7)</li> <li>\u5c0f\u4e09\u548c\u5f26\uff1a\u5c0f\u4e09\u5ea6+\u7eaf\u4e94\u5ea6\uff08\u4f8b\u5982Am\uff1aA C E\uff1bC-A=3\uff0cE-A=7\uff09</li> <li>\u5c5e\u4e03\u548c\u5f26\uff1a\u5927\u4e09\u5ea6+\u7eaf\u4e94\u5ea6+\u5c0f\u4e03\u5ea6(\u4f8b\u5982\\(G^{7}\\): G B D F; \u5206\u522b\u662f4 7 10)</li> <li>\u51cf\u4e09\u548c\u5f26\uff1a\u5c0f\u4e09\u5ea6+\u51cf\u4e94\u5ea6\uff08\u4f8b\u5982Bdim\uff1aB D F\uff1bD-B=3\uff0cF-B=6\uff09</li> <li>\u51cf\u4e03\u548c\u5f26\uff1a\u5c0f\u4e09\u5ea6+\u51cf\u4e94\u5ea6+\u51cf\u4e03\u5ea6</li> <li>\u534a\u51cf\u4e03\u548c\u5f26\uff1a\u5c0f\u4e09\u5ea6+\u51cf\u4e94\u5ea6+\u5c0f\u4e03\u5ea6\uff08\u4f8b\u5982\\(B^{\\varnothing7}\\) :B D F A; \u5bf9\u5e943 6 10)</li> <li>\u5927\u4e03\u548c\u5f26\uff1a\u5927\u4e09\u5ea6+\u7eaf\u4e94\u5ea6+\u5927\u4e03\u5ea6</li> <li>\u5927sus2\u548c\u5f26\uff1a\u5927\u4e8c\u5ea6+\u7eaf\u4e94\u5ea6</li> <li>\u5927sus4\u548c\u5f26\uff08\u5927sus\u548c\u5f26\uff09\uff1a\u7eaf\u56db\u5ea6+\u7eaf\u4e94\u5ea6</li> <li>\u5c5e9</li> </ol>"},{"location":"Notes/probability/","title":"Preface","text":""},{"location":"Notes/probability/#preface","title":"Preface","text":"<p>This is a set of notes for the Probability Theory and Mathematical Statistics course, Fall 2023, at ZJU. I\u2019ve long considered taking notes for this course but struggled to find an efficient method. Now, I believe Markdown and LaTeX are the perfect tools for the task. So, here it is - my first note written with Markdown and LaTeX. I will used English for the majority of this note to keep it concise and, in the process, familiarize myself with Academic English. Please forgive any inappropriate usage.</p> <p>12-17-2023</p> <p>notions:</p> <ul> <li> definition</li> <li> Theorem</li> <li> lemma</li> <li> important</li> <li> property</li> <li> corollary</li> </ul> <p>Get started with Basics</p>"},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/","title":"Basic of the Probability Theory","text":"<p> The law of total probability:</p> <p>Let \\(B_1, B_2, \\cdots, B_n\\) be a partition of S such that \\(P(B_i) &gt;0\\) for all  \\(i\\). Then for any events \\(A\\) in sample space \\(S\\),</p> \\[ \\displaystyle P(A)=\\sum P(A\\cap B_{n}) \\] <p>or alternatively,</p> \\[ \\displaystyle P(A)=\\sum P(A\\mid B_{n})P(B_{n}) \\] <p> Bayes' theorem: (Judging from the result, we try to find the probability of a cause)If \\(B_1, B_2, \\cdots, B_n\\) is a partition of \\(S\\). Then for any event \\(A\\) of the same sample space, if \\(P(A)\\neq 0\\) then we have:</p> \\[ {\\displaystyle P(B_{i}\\mid A)={\\frac {P(A\\mid B_{i})P(B_{i})}{\\sum _{j=1}^n P(A\\mid B_{j})P(B_{j})}}} \\quad i=1,2,3,\\ldots \\] Example <p>pass</p> <p> Let \\(S\\) be the sample space for an experiment. A real-valued, single-valued function that is defined on \\(S\\) is called a <code>random variable</code>(abbreviated r.v.). <code>Random Variables</code> are usually denoted by capital letters \\(X, Y, Z, \\cdots\\) and their values are denoted by the corresponding small letters \\(x, y, z, \\cdots\\).</p> Example <p>Think about rolling a dice. Let sample space \\(S=\\{1,2,3,4,5,6\\}\\). We define a random variable \\(X\\) = { the number of points on the upper face of the dice }.  Then \\(X\\) is a function from \\(S\\) to \\(\\mathbb{R}\\), i.e., \\(X: S \\rightarrow \\mathbb{R}\\).</p> <p>Distribution function:   Let \\(X\\) be a random variable defined on a sample space \\(S\\). The function \\(F(x)\\) defined by </p> \\[ F(x)=P(X\\leq x) \\] <p>is called the <code>distribution function</code> of \\(X\\). PS: for any real number \\(x_1, x_{2}(x_{1}&lt;x_{2})\\), we have </p> \\[ P(x_{1}&lt;X\\leq x_{2})=F(x_{2})-F(x_{1}) \\] <p>Intuitively, the distribution function \\(F(x)\\) gives the probability that the value of random variable \\(X\\) falls on \\((-\\infty,x)\\) </p> <p>Probability density function:   Let \\(X\\) be a Continuous random variableswith sample space \\(S\\) and distribution function \\(F(x)\\). The function \\(p(x)\\) defined by a none negative function \\(p(x)\\) such that</p> \\[ F(x)=\\int_{-\\infty}^{x}p(t)dt \\] <p>is called the <code>probability density function</code> of \\(X\\)(abbreviated p.d.f).</p> <p> It has the following properties:</p> <ol> <li>\\(f(x)\\ge 0\\)</li> <li>\\(\\int_{-\\infty}^{\\infty}f(x)dx=1\\)</li> <li>For any real number \\(x_1, x_{2}(x_{1}&lt;x_{2})\\), we have    \\(P(x_{1}&lt;X\\leq x_{2})=\\int_{x_{1}}^{x_{2}}f(x)dx\\) Intuitively, the area enclosed by the curve of the function with the x-axis and the two vertical lines \\(x_1\\) and \\(x_2\\) is the probability that the value of random variable \\(X\\) falls on \\((x_1,x_2)\\) For examples please check commom distributions</li> </ol> <p>Joint distribution function:   Let \\(X\\) and \\(Y\\) be two random variables defined on a sample space \\(S\\) and we treat \\((X,Y)\\) as the coordinate of a random point. The function \\(F(x,y)\\) defined by</p> \\[ F(x,y)=P(X\\leq x, Y\\leq y) \\] <p>is called the <code>joint distribution function</code> of \\(X\\) and \\(Y\\).</p> <p>Intuitively, the joint distribution function \\(F(x,y)\\) gives the probability that the value of random variable \\(X\\) falls on \\(\\{X&lt;x,Y&lt;y\\}\\)</p> <p>Marginal distribution function:   Let a bivariate random variable \\((X,Y)\\) have joint distribution function \\(F(x,y)\\), the marginal distribution function of \\(X,Y\\) are defined by</p> \\[ \\begin{aligned} F_{X}(x)=P(X\\leq x)=P(X\\leq x,Y&lt;\\infty)=F(x,\\infty)\\\\ F_{Y}(y)=P(Y\\leq y)=P(X&lt; \\infty,Y\\le y)=F(\\infty,y)\\\\ \\end{aligned} \\] <p>If \\(X,Y\\) are continous random variables, then we have: </p> \\[ \\begin{aligned} F_{X}(x)=P(X\\leq x)=P(X\\leq x,Y&lt;\\infty)=\\int_{-\\infty}^{x}\\left[ \\int_{-\\infty}^{\\infty}f(x,y)dy\\right]dx\\\\ F_{Y}(y)=P(Y\\leq y)=P(X&lt; \\infty,Y\\le y)=\\int_{-\\infty}^{\\infty}\\left[\\int_{-\\infty}^{y}f(x,y)dx\\right]dy\\\\ \\end{aligned} \\] <p>From equation above and the definition of p.d.f, we get the marginal p.d.f of \\(X,Y\\):</p> \\[ \\begin{aligned} f_{X}(x)=\\int_{-\\infty}^{\\infty}f(x,y)dy\\\\ f_{Y}(y)=\\int_{-\\infty}^{\\infty}f(x,y)dx\\\\ \\end{aligned} \\] <p>Conditional distribution function:  Let \\((X,Y)\\) be a bivariate continuous random variable, given any real number \\(x\\) ,if \\(P\\{x&lt;X&lt;x+\\delta\\}&gt;0 \\quad(\\delta&gt;0)\\), for any real number \\(y\\), the function \\(F_{Y\\mid X}(y\\mid x)\\) defined by</p> \\[ F_{Y\\mid X}(y\\mid x)=\\lim_{\\delta \\to 0^{+}}P(Y\\leq y\\mid x&lt;X&lt;x+\\delta) \\] <p>is called the <code>conditional distribution function</code> of \\(Y\\) given \\(X=x\\).</p> Attention <p>Usually, we use \\(P\\{Y&gt;y|X=x\\}\\) to denote \\(F_{Y\\mid X}(y\\mid x)\\). The probability here can't be calculated by the formula \\(P\\{Y&gt;y|X=x\\}=\\frac{P\\{Y&gt;y,X=x\\}}{P\\{X=x\\}}\\) because \\(P\\{X=x\\}=0\\). The proper way to do so is to get the conditional distribution function \\(F_{Y\\mid X}(y\\mid x)\\) first and we substitute \\(x\\) with its value to get the probability.</p> Example <p>pass</p> <p>Expectation of a discrete random variable:   Let \\(X\\) be a discrete random variable with sample space \\(S\\) and probability function \\(P(X=x_i)=p_i, i=1,2,\\cdots\\). The expectation of \\(X\\) is defined by</p> \\[ E(X)=\\sum_{i=1}^{\\infty}x_ip_i \\] Exception <p>If the series \\(\\sum_{i=1}^{\\infty}x_ip_i\\) is absolutely convergent, then \\(E(X)\\) exists. Otherwise, \\(E(X)\\) may not exist.</p> <p>Expectation of a continuous random variable:  Let \\(X\\) be a continuous random variable with sample space \\(S\\) and probability density function \\(f(x)\\). The expectation of \\(X\\) is defined by</p> \\[ E(X)=\\int_{-\\infty}^{\\infty}xf(x)dx \\] Exception <p>If the integral \\(\\int_{-\\infty}^{\\infty}\\left\\vert  x\\right\\vert f(x)dx\\) is finite, then \\(E(X)\\) exists. Otherwise, \\(E(X)\\) may not exist.</p> <p> Some properties of expectation \\(E(X)\\):  - If we have n Variables \\(X_1,X_2,\\cdots,X_n\\) which are indepentent with each other, then we have</p> \\[ E(\\prod_{n=1}^{\\infty} X_i)=\\prod_{n=1}^{\\infty}E(X_i) \\] <ul> <li>If \\(X\\) is a random variable and \\(a,b\\) are two constants, then we have</li> </ul> \\[ E(aX+b)=aE(X)+b \\] <ul> <li>If \\(X\\) and \\(Y\\) are two random variables, then we have</li> </ul> \\[ E(X+Y)=E(X)+E(Y) \\] <p>Variance of a discrete random variable: </p> \\[ \\operatorname{Var}(X)=E\\left[(X-E(X))^{2}\\right]=\\sum_{i=1}^{\\infty}(x_i-E(X))^2p_i \\] <p>Varaince of a continuous random variable: </p> \\[ \\operatorname{Var}(X)=E\\left[(X-E(X))^{2}\\right]=\\int_{-\\infty}^{\\infty}(x-E(X))^2f(x)dx \\] <p> Some properties of variance \\(\\operatorname{Var}(X)\\): </p> <ul> <li> \\[ \\operatorname{Var}(cX)=c^{2}\\operatorname{Var}(X) \\] <ul> <li> \\[ \\operatorname{Var}(X)=E(X^2)-[E(X)]^2 \\] <ul> <li>The equal sign holds if and only if \\(X=c\\).</li> </ul> \\[ \\operatorname{Var}(X)\\le E\\left[ (x-c)^2 \\right]\\] <p> Covariance of two random variables \\(X\\) and \\(Y\\) is defined by</p> \\[ \\operatorname{Cov}(X,Y)=E\\left[(X-E(X))(Y-E(Y))\\right] \\] <p>or equivalently(used for calcuation purpose),</p> \\[ \\operatorname{Cov}(X,Y)=E(XY)-E(X)E(Y) \\] Recall <p>\\(\\operatorname{Var}(X+Y)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)-2E\\left[(X-E(X))(Y-E(Y))\\right]\\)</p> Complement \\[ \\operatorname{Cov}(x,y)=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}\\left( x-E(X)\\right)\\left( y-E(Y)\\right)f(x,y)dxdy  \\] <p>We often use the definition above for proof purpose like some properties below.</p> <p> Some properties and theorem of covariance \\(\\operatorname{Cov}(X,Y)\\): </p> <ul> <li>Covariance is Symmetric:</li> </ul> \\[ \\operatorname{Cov}(X,Y)=\\operatorname{Cov}(Y,X) \\] <ul> <li>For any positive integer \\(n(n\\ge 2)\\), let \\(X_1, X_2, \\cdots, X_n\\) be \\(n\\) random variables whose variance exist, then \\(X_1, +X_2, \\cdots, +X_n\\)  also has variance and we have:</li> </ul> \\[ \\operatorname{Var}\\left(\\sum_{i=1}^{n} X_i\\right)=\\sum_{i=1}^{n} \\operatorname{Var}\\left(X_i\\right)+2 \\sum_{i&lt;j} \\operatorname{Cov}\\left(X_i, X_j\\right) \\] <ul> <li>Biliearity of Covariance:</li> </ul> \\[ \\operatorname{Cov}(aX+b,cY+d)=ac\\operatorname{Cov}(X,Y) \\] Generalization <p>More generally, we have:</p> \\[ \\operatorname{Cov}\\left(\\sum_{i=1}^{n} a_i X_i, \\sum_{j=1}^{m} b_j Y_j\\right)=\\sum_{i=1}^{n} \\sum_{j=1}^{m} a_{i} b_{j} \\operatorname{Cov}\\left(X_{i}, Y_{j}\\right) \\] <p>for example: we have \\(X_{1},X_{2},Y_{1},Y_{2}\\) and \\(a_{1},a_{2}.b_{1},b_{2}\\) corresponding to them. Then we have:</p> \\[ \\operatorname{Cov}\\left(a_{1} X_{1}+a_{2} X_{2}, b_{1} Y_{1}+b_{2} Y_{2}\\right)=a_{1} b_{1} \\operatorname{Cov}\\left(X_{1}, Y_{1}\\right)+a_{1} b_{2} \\operatorname{Cov}\\left(X_{1}, Y_{2}\\right)+a_{2} b_{1} \\operatorname{Cov}\\left(X_{2}, Y_{1}\\right)+a_{2} b_{2} \\operatorname{Cov}\\left(X_{2}, Y_{2}\\right) \\] <ul> <li>Addition rule :</li> </ul> \\[ \\operatorname{Cov}(X+Y,Z)=\\operatorname{Cov}(X,Z)+\\operatorname{Cov}(Y,Z) \\] <p> Correlation Coefficient \\(\\rho_{XY}\\) is defined by</p> \\[ \\rho_{XY}=\\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}} \\] Practice <p>Let \\(X^*=\\frac{X-E(X)}{\\sqrt {\\operatorname{Var}(X)}}\\), \\(Y^*=\\frac{Y-E(Y)}{\\sqrt {\\operatorname{Var}(Y)}}\\)</p> <p>use the properties above to prove:</p> \\[ \\rho_{XY}=\\operatorname{Cov}(X^*,Y^*) \\] <p>prove:</p> \\[ \\begin{aligned} \\operatorname{Cov}(X^*,Y^*) &amp;=\\operatorname{Cov}\\left(\\frac{X-E(X)}{\\sqrt {\\operatorname{Var}(X)}},\\frac{Y-E(Y)}{\\sqrt {\\operatorname{Var}(Y)}}\\right)\\\\ &amp;=\\frac{1}{\\sqrt {\\operatorname{Var}(X)\\operatorname{Var}(Y)}}\\operatorname{Cov}(X-E(X),Y-E(Y))\\\\ &amp;=\\frac{1}{\\sqrt {\\operatorname{Var}(X)\\operatorname{Var}(Y)}}\\operatorname{Cov}(X,Y)\\\\ \\end{aligned} \\] <p> Two random variables \\(X\\) and \\(Y\\) are said to be <code>uncorrelated</code> if \\(\\rho_{XY}=0\\).</p> <p>It is equivalent to say :</p> <ul> <li>\\(\\operatorname{Cov}(X,Y)=0\\)</li> <li>\\(E(XY)=E(X)E(Y)\\)</li> <li>\\(Var(X+Y)=Var(X)+Var(Y)\\)</li> </ul> Attention <p>Uncorrelated does not imply independence. In fact, if \\(X\\) and \\(Y\\) are independent, then they are uncorrelated. But the converse is not true. Namely:</p> \\[ \\operatorname{Cov}(X,Y)=0 \\nRightarrow X \\text { and } Y \\text { are independent } \\] \\[ X \\text { and } Y \\text { are independent } \\Rightarrow \\operatorname{Cov}(X,Y)=0 \\] Example <p>Let bianry random variable \\(X\\) and \\(Y\\) follow a uniform distribution within the region</p> \\[ D={(X,Y):x^{2}+y^{2}\\le r^{2}(r&gt;0)} \\] <p>Try to </p> <ol> <li>calculate \\(\\operatorname{Cov}(X,Y)\\) </li> <li>determine whether \\(X\\) and \\(Y\\) are independent, </li> <li>and whether they are uncorrelated.</li> </ol> <p>solution:</p> <p>The joint p.d.f of \\(X\\) and \\(Y\\) is:</p> \\[ f(x,y)= \\begin{cases}  \\frac{1}{\\pi r^{2}} \\quad &amp; x^{2}+y^{2} \\leq r^{2} \\\\  0&amp;  \\text{otherwise}\\end{cases}\\ \\] <p>So we have:</p> \\[ \\begin{aligned} E(X)&amp;=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xf(x,y)dxdy=0\\\\ E(Y)&amp;=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}yf(x,y)dxdy=0\\\\ E(XY)&amp;=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xyf(x,y)dxdy\\\\ &amp;=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}xy\\frac{1}{\\pi r^{2}}dxdy\\\\  &amp;=0\\\\ \\end{aligned} \\] <p>So we have:</p> \\[ \\operatorname{Cov}(X,Y)=E(XY)-E(X)E(Y)=0 \\] <p>So \\(X\\) and \\(Y\\) are uncorrelated. But they are not independent. </p> <p>Then we try to calculate the marginal p.d.f of \\(X\\) and \\(Y\\):</p> \\[ \\begin{aligned} f_{X}(x)&amp;=\\int_{-\\infty}^{\\infty}f(x,y)dy =\\int_{-\\sqrt{r^{2}-x^{2}}}^{\\sqrt{r^{2}-x^{2}}}\\frac{1}{\\pi r^{2}}dy =\\frac{2}{\\pi r^{2}}\\sqrt{r^{2}-x^{2}}\\\\ \\\\ f_{Y}(y)&amp;=\\int_{-\\infty}^{\\infty}f(x,y)dx=\\frac{2}{\\pi r^{2}}\\sqrt{r^{2}-y^{2}}\\\\ \\end{aligned} \\] <p>So we have:</p> \\[ f_{X}(x)f_{Y}(y)=\\frac{4}{\\pi^{2}r^{4}}(r^{2}-x^{2})(r^{2}-y^{2})\\neq f(x,y) \\] <p>So \\(X\\) and \\(Y\\) are not independent even though they are uncorrelated.</p> <p>Essentially, the correlation coefficient \\(\\rho_{XY}\\) measures the linear relationship between \\(X\\) and \\(Y\\). If \\(\\rho_{XY}=0\\), then \\(X\\) and \\(Y\\) are not linearly related. But they may be related in a nonlinear way.</p> <p> Let \\(X_1, X_2, \\cdots\\) be a sequence of random variables defined on a sample space \\(S\\). Let \\(X\\) be a random variable defined on the same sample space \\(S\\). We say that \\(X_n\\) converges to \\(X\\) in probability if for any \\(\\epsilon&gt;0\\), we have</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert X_{n}-X\\right\\vert \\geq \\epsilon\\right)=0 \\] <p>We write \\(X_n \\xrightarrow{P} X\\) ,\\(n \\to \\infty\\).</p> Example <p>Here is a simple example to illustrate the concept of convergence in probability. Let \\(X_1, X_2, \\cdots\\) be a sequence of random variables which follow a uniform distribution on the interval \\([0,1]\\). Let \\(X\\) be a random variable which follows a uniform distribution on the interval \\([0,1]\\). Then we have:</p> <p>Markov's inequality </p> <p> Let \\(X\\) be a random variable with \\(E[X^k]\\) exists. Then for any positive number \\(\\epsilon\\)(don't have to be a small number, can be any like 100 or 150), we have:</p> \\[ P\\left(\\left\\vert X\\right\\vert \\geq \\epsilon\\right) \\leq \\frac{E\\left[\\left\\vert X\\right\\vert ^{k}\\right]}{\\epsilon^{k}} \\] <p>or</p> \\[ P\\left(\\left\\vert X\\right\\vert \\leq \\epsilon\\right) \\geq 1- \\frac{E\\left[\\left\\vert X\\right\\vert ^{k}\\right]}{\\epsilon^{k}} \\] <p>To memorize the inequality, we can associate it with the cute emoji composed of the inequality signs and \\(\\epsilon\\) </p> \\[ \\geq \\epsilon \\leq \\] Example <p>The number of traffic accidents in a city in a year is a random variable \\(X\\) with \\(E[X]=1000\\). Use Markov's inequality to estimate the probability that the number of traffic accidents in the city in a year is at least 1500.</p> <p>solution:</p> \\[   P\\left(X \\geq 1500\\right) \\leq \\frac{E\\left[X\\right]}{1500}=\\frac{1000}{1500}=\\frac{2}{3} \\] <p>Chebyshev's inequality </p> <p> Let \\(X\\) be a random variable with variance and expection exist, we mark \\(E(X)=\\mu\\) and \\(Var(X)=\\sigma^2\\). Then for any positive number \\(\\epsilon&gt;0\\), we have:</p> \\[ P\\left(\\left\\vert X-\\mu\\right\\vert \\geq \\epsilon\\right) \\leq \\frac{\\sigma^{2}}{\\epsilon^{2}} \\] <p>or</p> \\[ P\\left(\\left\\vert X-\\mu\\right\\vert \\leq \\epsilon\\right) \\geq 1- \\frac{\\sigma^{2}}{\\epsilon^{2}} \\] <p>Look at the inequality above, we can see that the probability of \\(X\\) falling into the interval \\([\\mu-\\epsilon,\\mu+\\epsilon]\\) is at least \\(1-\\frac{\\sigma^{2}}{\\epsilon^{2}}\\). </p> <p>The reason why Chebyshev's inequality is important is that once we know the variance and expectation of a random variable, no matter what distribution it follows, we can use Chebyshev's inequality to estimate the probability that the random variable falls into a certain interval.</p> <p>We can also use that cute emoji to help memorize:</p> \\[ \\ge \\epsilon \\le \\]"},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#the-law-of-total-probability-and-bayes-theorem","title":"The Law of Total Probability and Bayes' Theorem","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#random-variable","title":"Random Variable","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#distribution-function-and-probability-density-function","title":"Distribution Function and Probability Density Function","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#joint-distribution-function-and-marginal-distribution-function","title":"Joint distribution function and Marginal distribution function","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#expectation","title":"Expectation","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#variance","title":"Variance","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#covariance","title":"Covariance","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#correlation-coefficient","title":"Correlation Coefficient","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#uncorrelated","title":"Uncorrelated","text":""},{"location":"Notes/probability/Basic%20of%20the%20Probability%20Theory/#convergence-in-probability","title":"Convergence in probability","text":""},{"location":"Notes/probability/Law%20of%20large%20numbers%20and%20central%20limit%20theorem/","title":"Law of large numbers and central limit theorem","text":"<p> Let \\(\\{X_i,i\\ge 1\\}\\) be a sequence of random variables, if a sequence of constant \\(\\{c_n, n\\ge 1\\}\\) exists such that to any \\(\\epsilon&gt;0\\), we have:</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert \\frac{1}{n}\\sum_{i=1}^{n}X_i-c_{n}\\right\\vert \\geq \\epsilon\\right)=0 \\] <p>or equivalently,</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert \\frac{1}{n}\\sum_{i=1}^{n}X_i-c_{n}\\right\\vert &lt; \\epsilon\\right)=1 \\] <p>then we say that \\(\\{X_i,i\\ge 1\\}\\) satisfies the <code>law of large number</code>.</p> <p>essentially, if \\(c_n=c\\), then we can write the law of large number as:</p> \\[ \\frac{1}{n}\\sum_{i=1}^{n}X_i \\xrightarrow{P} c, \\quad n \\to \\infty \\] <p>Bernoulli's law of large number </p> <p> Let \\(n_{A}\\) be the number of times that event \\(A\\) occurs in \\(n\\) independent trials of an experiment. Let \\(p=P(A)\\), then to any \\(\\epsilon&gt;0\\), we have:</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert \\frac{n_{A}}{n}-p\\right\\vert \\geq \\epsilon\\right)=0 \\] <p> Khintchine's law of large number  </p> <p> Let \\(X_1, X_2, \\cdots\\) be a sequence of independent random variables with \\(E(X_i)=\\mu\\). Then to any \\(\\epsilon&gt;0\\), we have:</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert \\frac{1}{n}\\sum_{i=1}^{n}X_i-\\mu\\right\\vert \\geq \\epsilon\\right)=0 \\] <p>namely:</p> \\[ \\frac{1}{n}\\sum_{i=1}^{n}X_i \\xrightarrow{P} \\mu, \\quad n \\to \\infty \\] <p>  An important corollary of Khintchine's law of large number  </p> <p>Let \\(X_1, X_2, \\cdots\\) be a sequence of independent random variables that follow the same distribution. \\(h(x)\\) is a continuous function and \\(E(|h(X)|)&lt;+\\infty\\) , then to any \\(\\epsilon&gt;0\\), we have:</p> \\[ \\lim_{n \\to \\infty}P\\left(\\left\\vert \\frac{1}{n}\\sum_{i=1}^{n}h(X_i)-E(h(X_i))\\right\\vert \\geq \\epsilon\\right)=0  \\] <p>Namely, we substitute \\(\\mu\\) with \\(E(h(X_i))\\). </p> Example <p>Let \\(X_1, X_2, \\cdots\\) be a sequence of independent random variables that follow the same distribution. and \\(\\operatorname{Var}(X_{i})=\\sigma^{2}\\), </p> <p>let:</p> \\[ S_{n}^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2} \\] <p>then we have:</p> \\[ S_n^{2}\\xrightarrow{P}\\sigma^{2}, \\quad n \\to \\infty \\] <p>solution:</p> \\[ S_{n}^{2}=\\frac{1}{n-1}(\\sum_{i=1}^{n}X_{i}^{2}-n\\overline{X}^{2}) \\] <p>use the corollary above, we have:</p> \\[ \\begin{aligned} \\overline{X}^{2}\\xrightarrow{P}\\mu_{1}^{2},\\quad n \\to \\infty\\\\ \\\\ \\frac{1}{n}\\sum_{i=1}^nX_i\\xrightarrow{P}E(X_{i}^{2})=\\mu_{2}\\quad n \\to  \\infty\\\\ \\end{aligned} \\] <p>And we have:</p> \\[ \\operatorname{Var}(X_{i})=E(X_{i}^{2})-[E(X_{i})]^{2}=\\mu_{2}-\\mu_{1}^{2} \\] <p>therefore:</p> \\[ \\begin{aligned} S_{n}^{2}&amp;=\\frac{1}{n-1}(\\sum_{i=1}^{n}X_{i}^{2}-n\\overline{X}^{2})=\\frac{n}{n-1}(\\mu_{2}-\\mu_{1}^{2})\\\\ &amp;\\xrightarrow{P}\\sigma^{2}, \\quad n \\to \\infty\\\\ \\end{aligned} \\] <p> Let \\(X_1, X_2, \\cdots\\) be a sequence of independent random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i)=\\sigma^2\\). Then for any real number \\(x\\), we have:</p> \\[ \\lim_{n \\to \\infty}P\\left(\\frac{\\sum_{i=1}^{n}X_i-n\\mu}{\\sigma\\sqrt{n}}\\leq x\\right)=\\Phi(x) \\] <p>where \\(\\Phi(x)\\) is the distribution function of the standard normal distribution.</p> <p>The theorem indicates that the normalized variable of \\(\\sum_{i=1}^{n}X_i\\) which is \\(\\frac{\\sum_{i=1}^{n}X_i-n\\mu}{\\sigma\\sqrt{n}}\\) approximately follows a standard normal distribution when \\(n\\) is large enough.</p> Example <p>Let \\(X_1, X_2, \\cdots\\) be a sequence of independent random variables that follow Bernoulli distribution with parameter n,p.Namely \\(X_n \\sim B(n,p)\\). Then we have:</p> \\[ \\sum_{i=1}^{n}X_n \\overset{\u8fd1\u4f3c\u5730}{\\sim } N(np,np(1-p)), \\quad n \\to \\infty \\]"},{"location":"Notes/probability/Law%20of%20large%20numbers%20and%20central%20limit%20theorem/#law-of-large-number","title":"Law of Large number","text":""},{"location":"Notes/probability/Law%20of%20large%20numbers%20and%20central%20limit%20theorem/#central-limit-theorem","title":"Central Limit Theorem","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/","title":"Mathemathical Statistics","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/#mathematical-statistics","title":"Mathematical Statistics","text":"<p> Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from a distribution with p.d.f \\(f(x;\\theta)\\), where \\(\\theta\\) is an unknown parameter. A statistic \\(T=T(X_1, X_2, \\cdots, X_n)\\) is called an <code>estimator</code> of \\(\\theta\\). The value \\(t=T(x_1, x_2, \\cdots, x_n)\\) is called an <code>estimate</code> of \\(\\theta\\).</p> <p>The definition above indicates that </p> <ul> <li>An estimator is a function of the random sample, no other unknown parameter is involved. </li> <li>An estimate is a value of the estimator when the random sample is observed.</li> </ul> <p>Below are two methods get an estimator:</p> <ul> <li>Method of moments</li> <li>Maximum Likelyhood Estimation</li> </ul> <p>When sample size \\(n\\) is large enough, the moment of the sample is approximately equal to the moment of the distribution. So we can use the moment of the sample to estimate the moment of the distribution. Namely:</p> \\[ \\begin{aligned} A_k \\xrightarrow{P} \\mu_k, \\quad n \\to \\infty\\\\ B_k \\xrightarrow{P} \\nu_k, \\quad n \\to \\infty\\\\ \\end{aligned} \\] Recall \\[ \\begin{aligned} \\mu_k = E[X^k] \\\\ \\nu_k = E[(X-\\mu)^k] \\end{aligned}  \\] <p>where \\(A_k\\) is the \\(k\\)th origin moment of the sample and \\(B_k\\) is the central moment of the sample. \\(\\mu_k\\) is the \\(k\\)th origin moment of the distribution and \\(\\nu_k\\) is the \\(k\\)th central moment of the distribution.</p> Example <p>Let population \\(X\\) follow a uniform distribution on the interval \\([a,b]\\). \\(X_{i}\\) are samples from \\(X\\). try to estimate \\(a\\) and \\(b\\).</p> <p>solution:</p> \\[ \\mu_{1}=\\frac{a+b}{2} \\quad \\nu_{2}=\\frac{(b-a)^{2}}{12}=S^{2} where S is the sample variance \\] <p>Therefore, </p> \\[ \\begin{aligned} a=\\mu_{1}-\\sqrt{3\\nu_{2}}\\\\ b=\\mu_{1}+\\sqrt{3\\nu_{2}}\\\\ \\end{aligned} \\] <p>use \\(A_{1}\\) and \\(B_{2}\\) to estimate \\(\\mu_{1}\\) and \\(\\nu_{2}\\), we have:</p> \\[ \\begin{aligned} \\hat{a}&amp;=\\overline{X}-\\sqrt{\\frac{3}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}\\\\ \\hat{b}&amp;=\\overline{X}+\\sqrt{\\frac{3}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}\\\\ \\end{aligned} \\] <p>Moment estimation is a very simple method. And the estimator we get is consistent, unbiased and efficient. It works even when the distribution of the population is unknown. </p> <p> Let population \\(X\\) follow a discrete distribution with p.m.f \\(f(x;\\theta)\\) Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from \\(X\\). Then the propobility of the  event \\({X=x}\\) is:</p> \\[ P_\\theta(X=x)=P_\\theta(X_1=x_1,X_2=x_2,\\cdots,X_n=x_n)=\\prod_{i=1}^{n}f(x_i;\\theta) \\] <p>It is a function of \\(\\theta\\) and \\(x_1, x_2, \\cdots, x_n\\). We call it the <code>likelihood function</code> of \\(\\theta\\) and denote it by \\(L(\\theta;\\mathbf{x})\\) or simply \\(L(\\theta)\\).</p> <p> Let population \\(X\\) follow a continuous distribution with p.d.f \\(f(x;\\theta)\\) Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from \\(X\\). Then the propobility of the  event \\({X=x}\\) is:</p> \\[ P(X=x)=\\prod_{i=1}^{n}f(x_i;\\theta) \\] <p>compare to the discrete case, the likelihood function can not be written as a probability of the event \\({X=x}\\).</p> <p>Maximum likelihood estimator:  Find a \\(\\hat{\\theta}\\) that makes:</p> \\[ L(\\hat{\\theta})=\\max_{\\theta \\in \\Theta}L(\\theta)  \\] <p>There are mainly two ways to do so:</p> <ul> <li>Take the derivative of \\(L(\\theta)\\) or \\(ln L(\\theta)\\) and let it equal to 0. Then we get \\(\\hat{\\theta}\\).</li> <li>When \\(L(\\theta)\\) is monotonic, all we have to do is to find the value range of \\(\\theta\\) usually determined by the relationship between \\(\\theta\\) and support of \\(X\\). </li> </ul> Example <p>Let population \\(X\\) has the given p.d.f:</p> \\[ f(x;\\theta)=\\begin{cases} \\frac{2x}{\\theta^{2}}, &amp; 0&lt;x&lt;\\theta \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <p>Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from \\(X\\) and \\(\\theta &gt;0\\). Find the maximum likelihood estimator of \\(\\theta\\) and the moment estimator of \\(\\theta\\).</p> <p>solution:</p> \\[ L(\\theta)=\\prod_{i=1}^{n}f(x_i;\\theta)=\\prod_{i=1}^{n}\\frac{2x_i}{\\theta^{2}}=\\frac{2^{n}}{\\theta^{2n}}\\prod_{i=1}^{n}x_i \\] <p>It's a monotocically decreasing function of \\(\\theta\\). And we have:</p> \\[ x_{(1)}&lt;= x_{(2)}&lt;= \\cdots, x_{(n)}\\le \\theta \\] <p>So we have:</p> \\[ \\hat{\\theta}=x_{(n)} \\] <p>where \\(x_{(n)}\\) is the maximum value of \\(x_1, x_2, \\cdots, x_n\\).</p> <p>We now try to get the moment estimator of \\(\\theta\\). We have:</p> \\[ \\mu_{1}=\\frac{2}{\\theta^{2}}\\int_{0}^{\\theta}x^{2}dx=\\frac{2}{\\theta^{2}}\\frac{\\theta^{3}}{3}=\\frac{2}{3}\\theta \\] <p>So we have:</p> \\[ \\hat{\\theta}=\\frac{3}{2}\\overline{X} \\] <p>where \\(\\overline{X}\\) is the sample mean.</p> <p>An important property of maximum likelihood estimation: the invariance property.   Let \\(\\hat{\\theta}\\) be the maximum likelihood estimator of \\(\\theta\\). Let \\(\\theta^*=g(\\theta)\\) be a continuous function of \\(\\theta\\). Then we have:</p> \\[ \\hat{\\theta}^*=g(\\hat{\\theta}) \\] <p>That is to say, once we get the maximum likelihood estimator of \\(\\theta\\), we can get the maximum likelihood estimator of any function of \\(\\theta\\).</p> <p>unbiasedness:   Let \\(\\hat{\\theta}\\) be an estimator of \\(\\theta\\). If \\(E(\\hat{\\theta})=\\theta\\), then we say that \\(\\hat{\\theta}\\) is an <code>unbiased estimator</code> of \\(\\theta\\).</p> Example <p>Try to prove that the variance of the sample \\(S^{2}\\) is an unbiased estimator of the variance of the population \\(\\sigma^{2}\\).</p> <p>solution:</p> \\[ \\begin{aligned} E\\left(S^{2}\\right)&amp;=E\\left(\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}\\right)\\\\ &amp;=\\frac{1}{n-1} E\\left(\\sum_{i=1}^{n} X_{i}^{2}-n \\overline{X}^{2}\\right)\\\\ &amp;=\\frac{1}{n-1} (n E\\left(X_{i}^{2}\\right)-n E\\left(\\overline{X}^{2}\\right))\\\\ \\end{aligned} \\] <p>Since \\(E\\left(X_{i}^{2}\\right)=\\sigma^{2}+\\mu_{1}^{2}\\) and \\(E\\left(\\overline{X}^{2}\\right)=\\frac{\\sigma^{2}}{n}+\\mu_{1}^{2}\\), we have:</p> \\[ \\begin{aligned} E\\left(S^{2}\\right)&amp;=\\frac{1}{n-1} (n (\\sigma^{2}+\\mu_{1}^{2})-n (\\frac{\\sigma^{2}}{n}+\\mu_{1}^{2}))\\\\ &amp;=\\frac{1}{n-1} (n-1) \\sigma^{2}\\\\ &amp;=\\sigma^{2}\\\\ \\end{aligned} \\] <p>efficiency:   Let \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) be two unbiased estimators of \\(\\theta\\). If \\(Var(\\hat{\\theta}_1)\\le Var(\\hat{\\theta}_2)\\), then we say that \\(\\hat{\\theta}_1\\) is more efficient than \\(\\hat{\\theta}_2\\).</p> <p>mean square error:   Let \\(\\hat{\\theta}\\) be an estimator of \\(\\theta\\). The <code>mean square error</code> of \\(\\hat{\\theta}\\) is defined by:</p> \\[ MSE(\\hat{\\theta})=E\\left[(\\hat{\\theta}-\\theta)^{2}\\right] \\] <p>If \\(MSE(\\hat{\\theta_{1}}) \\le  MSE(\\hat{\\theta_{2}})\\) then we say that \\(\\hat{\\theta_{1}}\\) is better than \\(\\hat{\\theta_{2}}\\) under the mean square error criterion.</p> Example <p>Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from a normal distribution \\(N(\\mu,\\sigma^{2})\\). Compare \\(S^{2}\\) and \\(B_{2}\\) under the mean square error criterion where \\(S^{2}\\) is the sample variance and \\(B_{2}\\) is the second central moment of the sample.</p> <p>solution: Since \\(S^{2}\\) is unbiased, we have:</p> \\[ \\begin{aligned} MSE\\left(S^{2}\\right)&amp;=E\\left[\\left(S^{2}-\\sigma^{2}\\right)^{2}\\right]\\\\ &amp;=E\\left[\\left(S^{2}-E\\left(S^{2}\\right)\\right)^{2}\\right]\\\\ &amp;=Var\\left(S^{2}\\right) \\end{aligned} \\] <p>And we know that:</p> \\[ \\frac{(n-1)S^{2}}{\\sigma^{2}} \\sim \\chi^{2}(n-1) \\] <p>So we have:</p> \\[ \\operatorname{Var}(S^{2})=\\frac{2 \\sigma^{4}}{n-1} \\] <p>For \\(B_{2}\\), we have:</p> \\[ \\begin{aligned} MSE\\left(B_{2}\\right)&amp;=E\\left[\\left(B_{2}-\\sigma^{2}\\right)^{2}\\right]\\\\ &amp;=E(B_2^{2})-2\\sigma^{2}E(B_2)+\\sigma^{4}\\\\ \\end{aligned} \\] <p>Since \\(B_{2} = \\frac{n-1}{n}S^{2}\\), we have:</p> \\[ \\begin{aligned} MSE\\left(B_{2}\\right)&amp;=E(B_2^{2})-2\\sigma^{2}E(B_2)+\\sigma^{4}\\\\ &amp;=\\frac{(n-1)^{2}}{n^{2}}E((S^{2})^{2}) - \\frac{2\\sigma^{2}(n-1)}{n}E(S^{2}) +\\sigma^{4}\\\\ &amp;=\\frac{(n-1)^{2}}{n^{2}}[\\operatorname{Var}(S^{2})+E^{2}(S^{2})]-\\frac{2\\sigma^{2}(n-1)}{n}E(S^{2})+\\sigma^{4}\\\\ &amp;=\\frac{(n-1)^{2}}{n^{2}}[\\frac{2 \\sigma^{4}}{n-1}+\\sigma^4]-\\frac{2\\sigma^{4}(n-1)}{n}+\\sigma^4\\\\ &amp;=\\frac{2n-1}{n^{2}}\\sigma^4 \\end{aligned} \\] <p>To any \\(n\\ge 2\\), \\(\\frac{2}{n-1}&gt;\\frac{2n-1}{n^{2}}\\). Thta is to say, \\(B_{2}\\) is better than \\(S^{2}\\) under the mean square error criterion.</p> <p>consistency:  The three methods above are all based on the assumption that the sample size \\(n\\) is determined. We may think, the larger the sample size, the better the estimator. So we need a new method to evaluate the estimator. That is the consistency.</p> <p> Let \\(\\hat{\\theta}\\) be an estimator of \\(\\theta\\). If to any \\(\\epsilon&gt;0\\), we have:</p> \\[ \\lim _{n \\rightarrow \\infty} P\\left(|\\hat{\\theta}-\\theta|&lt;\\epsilon\\right)=1 \\] <p>then we say that \\(\\hat{\\theta}\\) is a <code>consistent estimator</code> of \\(\\theta\\).</p> <p>Namely:</p> \\[ \\theta_{n} \\xrightarrow{P} \\theta, \\quad n \\to \\infty \\] Example <p>Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from a uniform distribution on the interval \\([0,\\theta]\\). we have the given estimators of \\(\\theta\\):</p> <ul> <li>Moment estimator: \\(\\hat{\\theta}_{1}=2 \\overline{X}\\)</li> <li>MLE :\\(\\hat{\\theta}_{2}=X_{(n)}\\)</li> <li>An unbiased estimator \\(\\hat{\\theta}_{3}=\\frac{n+1}{n} X_{(n)}\\)</li> </ul> <p>prove that: the three estimators are all consistent estimators of \\(\\theta\\).</p> <p>solution:</p> <p>According to the law of large numbers, we have:</p> \\[ \\overline{X} \\xrightarrow{P} \\frac{\\theta}{2}, \\quad n \\to \\infty \\] <p>Namely:</p> \\[ \\hat{\\theta}_{1}=2 \\overline{X} \\xrightarrow{P} \\theta, \\quad n \\to \\infty \\] <p>We now prove that \\(\\hat{\\theta}_{3}=\\frac{n+1}{n}X_{(n)}\\) is a consistent estimator of \\(\\theta\\).  We consider using chebyshev's inequality, so we need to get \\(A_{1}\\) and \\(B_{2}\\) We have:</p> \\[ F_{X_{(n)}}(x)=P\\left(X_{(n)} \\leq x\\right)=P\\left(X_{1} \\leq x, X_{2} \\leq x, \\cdots, X_{n} \\leq x\\right)=\\prod_{i=1}^{n} P\\left(X_{i} \\leq x\\right)=\\prod_{i=1}^{n} F_{X}(x)=\\left(\\frac{x}{\\theta}\\right)^{n} \\] <p>Namely:</p> \\[ f_{X_{(n)}}(x)=\\begin{cases} n\\frac{x^{n-1}}{\\theta^n}, &amp; 0&lt;x&lt;\\theta \\\\ 0, &amp;  \\text{otherwise}\\end{cases} \\] <p>So we have:</p> \\[ \\begin{aligned} A_{1}&amp;=E(X_{(n)})=\\int_{0}^{\\theta} x f_{X_{(n)}}(x) d x=\\int_{0}^{\\theta} x n\\frac{x^{n-1}}{\\theta^n} d x=\\frac{n}{n+1} \\theta\\\\ B_{2}&amp;=E\\left(X_{(n)}^{2}\\right)-A_{1}^{2}=\\int_{0}^{\\theta} x^{2} f_{X_{(n)}}(x) d x-\\left(\\frac{n}{n+1} \\theta\\right)^{2}=\\frac{n}{(n+1)^{2}(n+2)}\\theta^{2}\\\\ \\end{aligned} \\] <p>notice that, \\(A_{1}\\) and \\(B_{2}\\) are \\(\\hat{\\theta_{2}}'s\\) moment, so we need to get \\(A_{1}\\) and \\(B_{2}\\) of \\(\\hat{\\theta_{3}}\\). We have:</p> \\[ \\begin{aligned} A_{1}'&amp;=\\frac{n}{n+1}\\theta \\frac{n+1}{n} = \\theta\\\\ B_{2}'&amp;=\\frac{n}{(n+1)^{2}(n+2)}\\theta^{2}\\frac{(n+1)^{2}}{n^{2}}=\\frac{\\theta^{2}}{(n+2)n}\\\\ \\end{aligned} \\] <p>According to chebyshev's inequality, we have:</p> \\[ \\begin{aligned} P\\left(|\\hat{\\theta}_{3}-A_{2}'|\\ge \\epsilon\\right)&amp;\\le \\frac{B_{2}'}{\\epsilon^{2}} \\quad namely:\\\\ P\\left(|\\hat{\\theta}_{3}-\\theta|\\ge \\epsilon\\right)&amp;\\le \\frac{\\theta^{2}}{(n+2)n \\epsilon^{2}} \\rightarrow 0\\\\ \\end{aligned} \\] <p>According to squeeze theorem, </p> \\[ \\lim_{n \\to \\infty}P\\left(|\\hat{\\theta}_{3}-\\theta|\\ge \\epsilon\\right)=0 \\] <p>or</p> \\[ \\lim_{n \\to \\infty}P\\left(|\\hat{\\theta}_{3}-\\theta|\\le \\epsilon\\right)=1 \\] <p>Finally, we prove that \\(\\hat{\\theta_{2}}\\) is also consistent,</p> <p>If we treat \\(\\theta-\\hat{\\theta_{2}}\\) as a random variable, we have:</p> \\[ P\\{|\\theta-\\hat{\\theta_{2}}|&lt;\\epsilon\\}\\ge 1-\\frac{E(|\\theta-\\hat{\\theta_{2}}|)}{\\epsilon}=1-\\frac{E(\\theta-\\hat{\\theta_{2}})}{\\epsilon}=1-\\frac{\\theta}{(n+1)\\epsilon}\\rightarrow 1 \\] <p> Let \\(X_1, X_2, \\cdots, X_n\\) be a random sample from a distribution with p.d.f \\(f(x;\\theta)\\), where \\(\\theta\\) is an unknown parameter. Let \\(L(X_1, X_2, \\cdots, X_n)\\) and \\(U(X_1, X_2, \\cdots, X_n)\\) be two statistics such that:</p> \\[ P\\left(L(X_1, X_2, \\cdots, X_n)&lt;\\theta&lt;U(X_1, X_2, \\cdots, X_n)\\right)=1-\\alpha \\] <p>where \\(\\alpha\\) is a given number such that \\(0&lt;\\alpha&lt;1\\). Then the interval \\((L,U)\\) is called a <code>confidence interval</code> for \\(\\theta\\) with confidence coefficient \\(1-\\alpha\\). The probability \\(1-\\alpha\\) is called the <code>confidence level</code>.</p>"},{"location":"Notes/probability/Mathemathical%20Statistics/#point-estimation","title":"Point Estimation","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/#method-of-moments","title":"Method of Moments","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/#maximum-likelihood-estimation","title":"Maximum Likelihood Estimation","text":"<p>Likelihood function:  </p>"},{"location":"Notes/probability/Mathemathical%20Statistics/#methods-of-evaluating-estimators","title":"Methods of Evaluating Estimators","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/#confidence-intervals","title":"Confidence Intervals","text":""},{"location":"Notes/probability/Mathemathical%20Statistics/#pivotal-quantity","title":"Pivotal Quantity","text":""},{"location":"Notes/probability/common_distributions/","title":"Common Distributions","text":""},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/","title":"\u52a8\u8bcd\u53d8\u5316","text":"<p>\u65e5\u8bed\u7684\u52a8\u8bcd\u5206\u4e3a\u4e00\u7c7b\u52a8\u8bcd\uff08\u4e94\u6bb5\u52a8\u8bcd\uff09\u3001\u4e8c\u7c7b\u52a8\u8bcd\uff08\u4e0a\u4e00\u6bb5\u30fb\u4e0b\u4e00\u6bb5\u52a8\u8bcd\uff09\u548c\u4e09\u7c7b\u52a8\u8bcd\uff08\u4e0d\u89c4\u5219\u52a8\u8bcd\uff09\u3002\u6bcf\u4e00\u7c7b\u52a8\u8bcd\u7684\u6d3b\u7528\u89c4\u5219\u4e0d\u540c\uff0c\u4e0b\u9762\u8be6\u7ec6\u6574\u7406\u5b83\u4eec\u7684\u5404\u79cd\u5f62\u6001\u53d8\u5316\uff0c\u5305\u62ec\u7279\u6b8a\u60c5\u51b5\u548c\u4f8b\u5b50\u3002</p>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_1","title":"\u4e00\u7c7b\u52a8\u8bcd\uff08\u4e94\u6bb5\u52a8\u8bcd\uff09","text":""},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_2","title":"\u7279\u70b9","text":"<ul> <li>\u8bcd\u5c3e\u5047\u540d\u662f\u300c\u3046\u3001\u304f\u3001\u3050\u3001\u3059\u3001\u3064\u3001\u306c\u3001\u3076\u3001\u3080\u3001\u308b\u300d\u4e2d\u7684\u4e00\u4e2a\u3002</li> <li>\u53d8\u5316\u65f6\uff0c\u8bcd\u5e72\u7684\u6700\u540e\u4e00\u4e2a\u5047\u540d\u6309\u300c\u3042\u3001\u3044\u3001\u3046\u3001\u3048\u3001\u304a\u300d\u4e94\u6bb5\u6d3b\u7528\u3002</li> </ul>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_3","title":"\u5e38\u89c1\u8bcd\u4f8b","text":"\u8bcd\u5178\u5f62 \u307e\u3059\u5f62 \u3066\u5f62 \u305f\u5f62 \u5426\u5b9a\u5f62 \u610f\u5fd7\u5f62 \u547d\u4ee4\u5f62 \u53ef\u80fd\u5f62 \u88ab\u52a8\u5f62 \u4f7f\u5f79\u5f62 \u66f8\u304f \u66f8\u304d\u307e\u3059 \u66f8\u3044\u3066 \u66f8\u3044\u305f \u66f8\u304b\u306a\u3044 \u66f8\u3053\u3046 \u66f8\u3051 \u66f8\u3051\u308b \u66f8\u304b\u308c\u308b \u66f8\u304b\u305b\u308b \u98f2\u3080 \u98f2\u307f\u307e\u3059 \u98f2\u3093\u3067 \u98f2\u3093\u3060 \u98f2\u307e\u306a\u3044 \u98f2\u3082\u3046 \u98f2\u3081 \u98f2\u3081\u308b \u98f2\u307e\u308c\u308b \u98f2\u307e\u305b\u308b \u8a71\u3059 \u8a71\u3057\u307e\u3059 \u8a71\u3057\u3066 \u8a71\u3057\u305f \u8a71\u3055\u306a\u3044 \u8a71\u305d\u3046 \u8a71\u305b \u8a71\u305b\u308b \u8a71\u3055\u308c\u308b \u8a71\u3055\u305b\u308b \u6b7b\u306c \u6b7b\u306b\u307e\u3059 \u6b7b\u3093\u3067 \u6b7b\u3093\u3060 \u6b7b\u306a\u306a\u3044 \u6b7b\u306e\u3046 \u6b7b\u306d \u6b7b\u306d\u308b \u6b7b\u306a\u308c\u308b \u6b7b\u306a\u305b\u308b"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_4","title":"\u6d3b\u7528\u89c4\u5219","text":"<ol> <li>\u307e\u3059\u5f62\uff08\u656c\u4f53\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3044\u6bb5\u300d + \u307e\u3059  </li> <li>\u66f8\u304f \u2192 \u66f8\u304d\u307e\u3059</li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u3057\u307e\u3059</p> </li> <li> <p>\u3066\u5f62\uff08\u63a5\u7eed\u7528\u6cd5\uff09    \u89c4\u5219\uff1a</p> </li> <li>\u3046\u3001\u3064\u3001\u308b \u2192 \u3063\u3066\uff08ex: \u5f85\u3064 \u2192 \u5f85\u3063\u3066\uff09</li> <li>\u306c\u3001\u3076\u3001\u3080 \u2192 \u3093\u3067\uff08ex: \u98f2\u3080 \u2192 \u98f2\u3093\u3067\uff09</li> <li>\u304f \u2192 \u3044\u3066\uff08ex: \u66f8\u304f \u2192 \u66f8\u3044\u3066\uff09\u4f46\u300c\u884c\u304f\u300d\u4f8b\u5916\uff0c\u4e3a\u300c\u884c\u3063\u3066\u300d</li> <li>\u3050 \u2192 \u3044\u3067\uff08ex: \u6cf3\u3050 \u2192 \u6cf3\u3044\u3067\uff09</li> <li> <p>\u3059 \u2192 \u3057\u3066\uff08ex: \u8a71\u3059 \u2192 \u8a71\u3057\u3066\uff09</p> </li> <li> <p>\u305f\u5f62\uff08\u8fc7\u53bb\u5f0f\uff0c\u4e0e\u3066\u5f62\u76f8\u540c\uff09  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u3044\u305f </li> <li> <p>\u98f2\u3080 \u2192 \u98f2\u3093\u3060</p> </li> <li> <p>\u5426\u5b9a\u5f62\uff08\u306a\u3044\u5f62\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3042\u6bb5\u300d+ \u306a\u3044  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u304b\u306a\u3044 </li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u3055\u306a\u3044</p> </li> <li> <p>\u610f\u5fd7\u5f62\uff08\u8ba9\u6211\u4eec\u505a...\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u304a\u6bb5\u300d+ \u3046  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u3053\u3046 </li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u305d\u3046</p> </li> <li> <p>\u547d\u4ee4\u5f62\uff08\u547d\u4ee4\u5f0f\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3048\u6bb5\u300d  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u3051 </li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u305b</p> </li> <li> <p>\u53ef\u80fd\u5f62\uff08\u80fd\u505a\u67d0\u4e8b\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3048\u6bb5\u300d+ \u308b  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u3051\u308b </li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u305b\u308b</p> </li> <li> <p>\u88ab\u52a8\u5f62\uff08\u53d7\u8eab\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3042\u6bb5\u300d+ \u308c\u308b  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u304b\u308c\u308b </li> <li> <p>\u8a71\u3059 \u2192 \u8a71\u3055\u308c\u308b</p> </li> <li> <p>\u4f7f\u5f79\u5f62\uff08\u8ba9\u67d0\u4eba\u505a\uff09    \u8bcd\u5c3e\u300c\u3046\u6bb5\u300d\u2192\u300c\u3042\u6bb5\u300d+ \u305b\u308b  </p> </li> <li>\u66f8\u304f \u2192 \u66f8\u304b\u305b\u308b </li> <li>\u8a71\u3059 \u2192 \u8a71\u3055\u305b\u308b</li> </ol>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_5","title":"\u4e8c\u7c7b\u52a8\u8bcd\uff08\u4e0a\u4e00\u6bb5\u30fb\u4e0b\u4e00\u6bb5\u52a8\u8bcd\uff09","text":""},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_6","title":"\u7279\u70b9","text":"<ul> <li>\u8bcd\u5c3e\u5047\u540d\u662f\u300c\u308b\u300d\uff0c\u4e14\u524d\u4e00\u4e2a\u5047\u540d\u4e3a\u300c\u3044\u300d\u6216\u300c\u3048\u300d\u6bb5\u97f3\u3002</li> <li>\u8bcd\u5e72\u56fa\u5b9a\u4e0d\u53d8\uff0c\u76f4\u63a5\u52a0\u540e\u7f00\u3002</li> </ul>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_7","title":"\u5e38\u89c1\u8bcd\u4f8b","text":"\u8bcd\u5178\u5f62 \u307e\u3059\u5f62 \u3066\u5f62 \u305f\u5f62 \u5426\u5b9a\u5f62 \u610f\u5fd7\u5f62 \u547d\u4ee4\u5f62 \u53ef\u80fd\u5f62 \u88ab\u52a8\u5f62 \u4f7f\u5f79\u5f62 \u98df\u3079\u308b \u98df\u3079\u307e\u3059 \u98df\u3079\u3066 \u98df\u3079\u305f \u98df\u3079\u306a\u3044 \u98df\u3079\u3088\u3046 \u98df\u3079\u308d \u98df\u3079\u3089\u308c\u308b \u98df\u3079\u3089\u308c\u308b \u98df\u3079\u3055\u305b\u308b \u898b\u308b \u898b\u307e\u3059 \u898b\u3066 \u898b\u305f \u898b\u306a\u3044 \u898b\u3088\u3046 \u898b\u308d \u898b\u3089\u308c\u308b \u898b\u3089\u308c\u308b \u898b\u3055\u305b\u308b"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_8","title":"\u6d3b\u7528\u89c4\u5219","text":"<ol> <li>\u307e\u3059\u5f62\uff1a\u76f4\u63a5\u53bb\u6389\u300c\u308b\u300d+ \u307e\u3059 </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u307e\u3059</p> </li> <li> <p>\u3066\u5f62\u3001\u305f\u5f62\uff1a\u76f4\u63a5\u53bb\u6389\u300c\u308b\u300d+ \u3066/\u305f </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u3066\uff0c\u98df\u3079\u305f</p> </li> <li> <p>\u5426\u5b9a\u5f62\uff1a\u53bb\u6389\u300c\u308b\u300d+ \u306a\u3044 </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u306a\u3044</p> </li> <li> <p>\u610f\u5fd7\u5f62\uff1a\u53bb\u6389\u300c\u308b\u300d+ \u3088\u3046 </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u3088\u3046</p> </li> <li> <p>\u547d\u4ee4\u5f62\uff1a\u53bb\u6389\u300c\u308b\u300d+ \u308d \u6216 \u3088 </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u308d</p> </li> <li> <p>\u53ef\u80fd\u5f62\uff1a\u53bb\u6389\u300c\u308b\u300d+ \u3089\u308c\u308b </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u3089\u308c\u308b\uff08\u53e3\u8bed\u4e2d\u4e5f\u4f1a\u7b80\u5316\u4e3a \u98df\u3079\u308c\u308b\uff09</p> </li> <li> <p>\u88ab\u52a8\u5f62\uff1a\u540c\u53ef\u80fd\u5f62  </p> </li> <li> <p>\u98df\u3079\u308b \u2192 \u98df\u3079\u3089\u308c\u308b</p> </li> <li> <p>\u4f7f\u5f79\u5f62\uff1a\u53bb\u6389\u300c\u308b\u300d+ \u3055\u305b\u308b </p> </li> <li>\u98df\u3079\u308b \u2192 \u98df\u3079\u3055\u305b\u308b</li> </ol>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_9","title":"\u4e09\u7c7b\u52a8\u8bcd\uff08\u4e0d\u89c4\u5219\u52a8\u8bcd\uff09","text":""},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_10","title":"\u7279\u70b9","text":"<ul> <li>\u53ea\u6709\u4e24\u4e2a\u52a8\u8bcd\uff1a\u300c\u3059\u308b\u300d\u548c\u300c\u6765\u308b\u300d\uff0c\u53d8\u5316\u5b8c\u5168\u4e0d\u89c4\u5219\u3002</li> </ul>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_11","title":"\u6d3b\u7528\u89c4\u5219","text":"\u8bcd\u5178\u5f62 \u307e\u3059\u5f62 \u3066\u5f62 \u305f\u5f62 \u5426\u5b9a\u5f62 \u610f\u5fd7\u5f62 \u547d\u4ee4\u5f62 \u53ef\u80fd\u5f62 \u88ab\u52a8\u5f62 \u4f7f\u5f79\u5f62 \u3059\u308b \u3057\u307e\u3059 \u3057\u3066 \u3057\u305f \u3057\u306a\u3044 \u3057\u3088\u3046 \u3057\u308d / \u305b\u3088 \u3067\u304d\u308b \u3055\u308c\u308b \u3055\u305b\u308b \u6765\u308b \u6765\u307e\u3059 \u6765\u3066 \u6765\u305f \u6765\u306a\u3044 \u6765\u3088\u3046 \u6765\u3044 \u6765\u3089\u308c\u308b \u6765\u3089\u308c\u308b \u6765\u3055\u305b\u308b"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%8A%A8%E8%AF%8D%E5%8F%98%E5%8C%96/#_12","title":"\u7279\u6b8a\u60c5\u51b5","text":"<ol> <li>\u3059\u308b \u7684\u590d\u5408\u52a8\u8bcd</li> <li>\u52c9\u5f37\u3059\u308b \u2192 \u52c9\u5f37\u3057\u307e\u3059\uff0c\u52c9\u5f37\u3057\u3066\uff0c\u52c9\u5f37\u3057\u305f</li> <li> <p>\u7814\u7a76\u3059\u308b \u2192 \u7814\u7a76\u3057\u307e\u3059\uff0c\u7814\u7a76\u3057\u3066\uff0c\u7814\u7a76\u3057\u305f</p> </li> <li> <p>\u6765\u308b \u53d8\u5316</p> </li> <li>\u307e\u3059\u5f62 \u2192 \u6765\u307e\u3059</li> <li>\u3066\u5f62 \u2192 \u6765\u3066</li> <li>\u53ef\u80fd\u5f62 \u2192 \u6765\u3089\u308c\u308b\uff08\u53e3\u8bed\u5e38\u7b80\u5316\u4e3a\u300c\u6765\u308c\u308b\u300d\uff09</li> </ol> <p>\u7ec3\u4e60\uff1a\u5e38\u7528\u52a8\u8bcd\u53d8\u578b | \u8bcd\u5178\u5f62 | \u307e\u3059\u5f62 | \u3066\u5f62 | \u305f\u5f62 | \u5426\u5b9a\u5f62 | \u610f\u5fd7\u5f62 | \u547d\u4ee4\u5f62 | \u53ef\u80fd\u5f62 | \u88ab\u52a8\u5f62 | \u4f7f\u5f79\u5f62 | | ------ | ------ | ---- | ---- | ------ | ------ | ------ | ------ | ------ | ------ | | \u9055\u3046 | \u9055\u3044\u307e\u3059 </p>"},{"location":"Notes/%E6%97%A5%E6%9C%AC%E8%AA%9E/%E5%B8%B8%E8%A7%81%E6%90%AD%E9%85%8D%E2%80%94%E2%80%94%E6%9F%90%E4%BA%BA%E4%B8%80%E5%A4%A9%E7%9A%84%E7%94%9F%E6%B4%BB/","title":"\u5e38\u89c1\u642d\u914d\u2014\u2014\u67d0\u4eba\u4e00\u5929\u7684\u751f\u6d3b","text":"<ul> <li>\u8d77\u304d\u307e\u3059</li> <li> <p>\u7a93\u3092\u3000\u958b\u3051\u307e\u3059\u3000\u3000\u3000\uff08\u307e\u3069\u3092\u3000\u3042\u3051\u307e\u3059\uff09</p> </li> <li> <p>\u9854\u3092\u3000\u6d17\u3044\u307e\u3059\u3000\u3000\u3000\uff08\u304b\u304a\u3092\u3000\u3042\u3089\u3044\u307e\u3059\uff09</p> </li> <li>\u6b6f\u3092\u3000\u78e8\u304d\u307e\u3059\u3000\u3000\u3000\uff08\u306f\u3092\u3000\u307f\u304c\u304d\u307e\u3059\uff09</li> <li>\u9aea\u3092\u3000\u3068\u304b\u3057\u307e\u3059\u3000\u3000\uff08\u304b\u307f\u3092\u3000\u3068\u304b\u3057\u307e\u3059\uff09</li> <li> <p>\u670d\u3092\u3000\u7740\u307e\u3059\u3000\u3000\u3000\u3000\uff08\u3075\u304f\u3092\u3000\u304d\u307e\u3059\uff09</p> </li> <li> <p>\u304a\u8336\u3092\u3000\u5165\u308c\u307e\u3059\u3000\u3000\uff08\u304a\u3061\u3083\u3092\u3000\u3044\u308c\u307e\u3059\uff09</p> </li> <li>\u304a\u8336\u3092\u3000\u98f2\u307f\u307e\u3059\u3000\u3000\uff08\u304a\u3061\u3083\u3092\u3000\u306e\u307f\u307e\u3059\uff09</li> <li>\u3054\u98ef\u3092\u3000\u98df\u3079\u307e\u3059\u3000\u3000\uff08\u3054\u306f\u3093\u3092\u3000\u305f\u3079\u307e\u3059\uff09</li> <li>\u30bf\u30d0\u30b3\u3092\u3000\u5438\u3044\u307e\u3059\u3000\uff08\u305f\u3070\u3053\u3092\u3000\u3059\u3044\u307e\u3059\uff09</li> <li> <p>\u65b0\u805e\u3092\u3000\u8aad\u307f\u307e\u3059\u3000\u3000\uff08\u3057\u3093\u3076\u3093\u3092\u3000\u3088\u307f\u307e\u3059\uff09</p> </li> <li> <p>\u9774\u3092\u3000\u5c65\u304d\u307e\u3059\u3000\uff08\u304f\u3064\u3092\u306f\u304d\u307e\u3059\uff09</p> </li> <li>\u30c9\u30a2\u3092\u9589\u3081\u307e\u3059\u3000\uff08\u30c9\u30a2\u3092\u3057\u3081\u307e\u3059\uff09</li> <li>\u9375\u3092\u3000\u304b\u3051\u307e\u3059\u3000\uff08\u304b\u304e\u3092\u3000\u304b\u3051\u307e\u3059\uff09\u3000\u9501\u95e8</li> <li> <p>\u3054\u307f\u3092\u51fa\u3057\u307e\u3059\u3000\uff08\u3054\u307f\u3092\u3060\u3057\u307e\u3059\uff09    \u6254\u5783\u573e</p> </li> <li> <p>\u4f1a\u793e\u3078\u3000\u884c\u304d\u307e\u3059\u3000\uff08\u304b\u3044\u3057\u3083\u3078\u3000\u3044\u304d\u307e\u3059\uff09</p> </li> <li>\u5207\u7b26\u3092\u3000\u8cb7\u3044\u307e\u3059\u3000\uff08\u304d\u3063\u3077\u3092\u3000\u304b\u3044\u307e\u3059\uff09  \u4e70\u90ae\u7968</li> <li>\u96d1\u8a8c\u3092\u3000\u8aad\u307f\u307e\u3059\u3000\uff08\u3056\u3063\u3057\u3092\u3000\u3088\u307f\u307e\u3059\uff09</li> <li> <p>\u97f3\u697d\u3092\u3000\u805e\u304d\u307e\u3059\u3000\uff08\u304a\u3093\u304c\u304f\u3092\u3000\u304d\u304d\u307e\u3059\uff09</p> </li> <li> <p>\u30e1\u30fc\u30eb\u3092\u3000\u51fa\u3057\u307e\u3059\u3000\uff08\u30e1\u30fc\u30eb\u3092\u3060\u3057\u307e\u3059\uff09\u3000\u53d1\u90ae\u4ef6</p> </li> <li>\u624b\u7d19\u3092\u3000\u66f8\u304d\u307e\u3059\u3000\u3000\uff08\u3066\u304c\u307f\u3092\u304b\u304d\u307e\u3059\uff09\u3000\u5199\u4fe1</li> <li>\u5207\u624b\u3092\u3000\u8cbc\u308a\u307e\u3059\u3000\u3000\uff08\u304d\u3063\u3066\u3092\u306f\u30ea\u307e\u3059\uff09\u3000\u8d34\u90ae\u7968</li> <li>\u96fb\u8a71\u3092\u3000\u304b\u3051\u307e\u3059\u3000\u3000\uff08\u3067\u3093\u308f\u3092\u304b\u3051\u307e\u3059\uff09\u3000\u6253\u7535\u8bdd</li> <li>\u30d5\u30a2\u30c3\u30af\u30b9\u3092\u3000\u9001\u308a\u307e\u3059\u3000\uff08\u30d5\u30a1\u30c3\u30af\u30b9\u3092\u3000\u304a\u304f\u308a\u307e\u3059\uff09\u3000\u4f20\u771f</li> <li> <p>\u8cc7\u6599\u3092\u3000\u4f5c\u308a\u307e\u3059\u3000\u3000\uff08\u3057\u308a\u3087\u3046\u3092\u3000\u3064\u304f\u308a\u307e\u3059\uff09\u3000\u5236\u4f5c\u8d44\u6599</p> </li> <li> <p>\u9280\u884c\u3078\u3000\u884c\u304d\u307e\u3059\u3000\uff08\u304e\u3093\u3053\u3046\u3078\u3000\u3044\u304d\u307e\u3059\uff09</p> </li> <li>\u304a\u91d1\u3092\u3000\u6255\u3044\u307e\u3059\u3000\uff08\u304a\u304b\u306d\u3092\u3000\u306f\u3089\u3044\u307e\u3059\uff09\u3000\u4ed8\u94b1</li> <li>\u8eca\u3092\u3000\u6d17\u3044\u307e\u3059\u3000\u3000\uff08\u304f\u308b\u307e\u3092\u3000\u3042\u3089\u3044\u307e\u3059\uff09\u3000\u6d17\u8f66</li> <li>\u5199\u771f\u3092\u3000\u64ae\u308a\u307e\u3059\u3000\uff08\u3057\u3083\u3057\u3093\u3092\u3000\u3068\u308a\u307e\u3059\uff09\u3000\u62cd\u7167</li> <li>\u5bb6\u3078\u3000\u5e30\u308a\u307e\u3059\u3000\u3000\uff08\u3044\u3048\u3078\u3000\u304b\u3048\u308a\u307e\u3059\uff09\u3000\u56de\u5bb6</li> <li>\u9375\u3092\u3000\u958b\u3051\u307e\u3059\u3000\u3000\uff08\u304b\u304e\u3092\u3000\u3042\u3051\u307e\u3059\uff09\u3000\u5f00\u9501</li> <li>\u30c9\u30a2\u3092\u3000\u958b\u3051\u307e\u3059\u3000\uff08\u30c9\u30a2\u3092\u3000\u3042\u3051\u307e\u3059\uff09\u3000\u5f00\u95e8</li> <li>\u9774\u3092\u3000\u8131\u304e\u307e\u3059\u3000\u3000\uff08\u304f\u3064\u3092\u3000\u306c\u304e\u307e\u3059\uff09\u3000\u8131\u978b\u3000</li> <li> <p>\u670d\u3092\u3000\u8131\u304e\u307e\u3059\u3000\u3000\uff08\u3075\u304f\u3092\u3000\u306c\u304e\u307e\u3059\uff09\u3000\u8131\u8863\u670d</p> </li> <li> <p>\u96fb\u6c17\u3092\u3000\u3064\u3051\u307e\u3059\u3000\u3000\uff08\u3066\u3093\u304d\u3092\u3000\u3064\u3051\u307e\u3059\uff09\u3000\u5f00\u706f</p> </li> <li>\u30c6\u30ec\u30d3\u3092\u3000\u3064\u3051\u307e\u3059\u3000\uff08\u30c6\u30ec\u30d3\u3092\u3000\u3064\u3051\u307e\u3059\uff09\u3000\u5f00\u7535\u89c6</li> <li>\u30c6\u30ec\u30d3\u3092\u898b\u307e\u3059\u3000\u3000\u3000\uff08\u30c6\u30ec\u30d3\u3092\u3000\u307f\u307e\u3059\uff09\u3000\u3000\u770b\u7535\u89c6</li> <li> <p>\u6599\u7406\u3092\u3000\u4f5c\u308a\u307e\u3059\u3000\u3000\uff08\u308a\u3087\u3046\u308a\u3092\u3000\u3064\u304f\u308a\u307e\u3059\uff09\u3000\u505a\u996d</p> </li> <li> <p>\u30b7\u30e3\u30ef\u30fc\u3092\u3000\u6d74\u3073\u307e\u3059\u3000\uff08\u30b7\u30e3\u30ef\u30fc\u3092\u3042\u3073\u307e\u3059\uff09\u3000\u6d17\u6fa1</p> </li> <li>\u4f53\u3092\u3000\u6d17\u3044\u307e\u3059\u3000\u3000\u3000\u3000\uff08\u304b\u3089\u3060\u3092\u3042\u3089\u3044\u307e\u3059\uff09\u3000\u6d17\u8eab\u4f53</li> <li> <p>\u982d\u3092\u3000\u6d17\u3044\u307e\u3059\u3000\u3000\u3000\u3000\uff08\u3042\u305f\u307e\u3092\u3042\u3089\u3044\u307e\u3059\uff09\u3000\u6d17\u5934</p> </li> <li> <p>\u7a93\u3092\u3000\u9589\u3081\u307e\u3059\u3000\u3000\u3000\uff08\u307e\u3069\u3092\u3057\u3081\u307e\u3059\uff09\u3000\u5173\u7a97</p> </li> <li>\u96fb\u6c17\u304d\u3092\u3000\u6d88\u3057\u307e\u3059\u3000\uff08\u3067\u3093\u304d\u3092\u3051\u3057\u307e\u3059\uff09\u3000\u5173\u706f</li> <li>\u5bdd\u307e\u3059\u3000\u3000\u3000\u3000\u3000\u3000\u3000\uff08\u306d\u307e\u3059\uff09\u3000\u7761\u89c9</li> </ul>"},{"location":"SG/Food%20SG/","title":"Food Tasting in NTU","text":""},{"location":"SG/Food%20SG/#food-tasting-in-ntu","title":"Food Tasting in NTU","text":"<p>So, I\u2019m trying to improve my English by writing down random stuff about my life in Singapore. This includes anything from thoughts I have, cool food I try, or just whatever pops into my head. First up\u2014food! Because, why not?</p> <p>I\u2019ll keep this updated with the food I try at NTU, giving each dish a rating from 1 to 5 stars. I\u2019ll also throw in some prices, where you can get it, and how I feel about the taste.</p>"},{"location":"SG/Food%20SG/#canteen-11","title":"Canteen 11","text":"<ol> <li> <p>Mix Rice and Veggies </p> <ul> <li>Price: Around $3-$5</li> <li>Recommended: Broccoli-cauliflower mix, sweet and sour ribs.</li> <li>Review: Not sure what to eat? Mix rice is always a safe bet. You get to choose from over 15 different options of veggies and meat. It\u2019s your typical Chinese food, but they have my favorite broccoli every day. And honestly, the other dishes are even better than the ones I had back in China.   PS: I've tried so much Mix rice and veggie in singapore and really, couldn't tell which one really impressed me. In a daily senario, go for the nearest one can save you time and avoid suffering from making decisions.</li> </ul> </li> <li> <p>Japanese Food </p> <ul> <li>Price: $5-$7</li> <li>Not Recommended</li> <li>Review: I haven\u2019t tried everything on the menu, but I went for the fried chicken don on a friend\u2019s recommendation. The soft-boiled egg was nice, but the chicken? Way too greasy and missing some veggies. I also tried the fried shrimp don for $6.50, which wasn\u2019t terrible, but not worth the price.</li> </ul> </li> <li> <p>Malay Food </p> <ul> <li>Price: $2.50-$3.50</li> <li>Recommended: Briyani, Nasi Lemak</li> <li>Review: On a tight budget? Malay food is cheap and\u2026 average. The oily rice and fried chicken with spicy Malay sauce will fill you up, but there\u2019s barely any veggies other than cucumber, so not the most balanced meal.</li> </ul> </li> <li> <p>Fruit </p> <ul> <li>Price: $1-$2</li> <li>Recommended: Papaya</li> <li>Review: After a heavy meal, some fruit is the perfect way to refresh your mouth and mood! The auntie at the fruit stall is super nice. Prices are a bit higher than at the wet market, but hey, the convenience is worth it.</li> </ul> </li> </ol>"},{"location":"SG/Food%20SG/#saraca-canteen","title":"Saraca Canteen","text":"<ol> <li> <p>Indian Food </p> <ul> <li>Price: $4.50-$8</li> <li>Recommended: Briyani, Naan, potatoes</li> <li>Review: Since I landed in Singapore and had my first meal\u2014Briyani and Mee Goreng\u2014I\u2019ve been hooked on Indian food. I love how they fry the rice with all those spices, and the curry chicken always hits the spot. It\u2019s a bit spicy, which is a plus for me. The naan is also a must-try\u2014so chewy and perfect for dipping in curry.   Oh, and don\u2019t miss the potatoes! Not exactly sure how they cook them, but they\u2019re chunked with the skin on, fried with spices until they\u2019re soft and mashed. It\u2019s an amazing, flavorful side dish.</li> </ul> </li> <li> <p>Fruits and Drinks </p> <ul> <li>Price: $0.90-$3.50</li> <li>Recommended: Avocado milkshake</li> <li>Review: I\u2019m obsessed with the avocado milkshake! It\u2019s thick, creamy, and has that strong avocado taste. At $3, it's a great dessert option after a meal. But fair warning: not everyone is a fan of avocado\u2019s fatty texture. My roommate\u2019s not into it at all.</li> </ul> </li> <li> <p>Mixed Rice and Veggie </p> <ul> <li>Price: $4.5-$6</li> <li>Recommended: Fresh broccoli, stewed beef with potatoes, mala fish, scrambled eggs with tomatoes</li> <li>Review: The best mixed rice and veggie in NTU. I usually have lunch or more precisely brunch.(Since I often get up after 11:30). Order broccoli, beef and eggs for 5 SGD, grab a cup of ice milo and then take the red campus loop to the iot lab. It has been my recent routine. Let's take about the food taste. The best part of the broccoli is that the cook really keeps it cruchy and fresh, not too well nor too raw, just right the way I like. The beef is chewy yet not too hard, and the potatoes are soft and flavorful. </li> </ul> </li> </ol>"},{"location":"SG/Food%20SG/#north-hill-canteen","title":"North hill Canteen","text":"<ol> <li> <p>Taiwanese Food </p> <ul> <li>Price: $4-$5</li> <li>Recommended: Braised pork rice</li> <li>Review: I\u2019ve only tried the braised pork rice, but it\u2019s so good! The pork is tender and flavorful. Mix it with the rice and you'll get a super satisfying meal. Portion is the only problem-only get me 80% full.</li> </ul> </li> <li> <p>Thai Food </p> <ul> <li>Price: $4-$6</li> <li>Recommended: Green curry Fried Rice</li> <li>Review: : It serves a variety of Thai dishes, I've tried the green curry fried rice and Fried Hefen. As for the fried hefen, I don't see too much difference from the Chinese version. The green curry fried rice is Okay but still, where's the taste of green curry?</li> </ul> </li> <li> <p>Korean Food</p> <ul> <li>Price: $4-$8</li> <li>Recommended: Kimchi Fried Rice</li> <li>Review: : Kimchi fried rice there has a decent taste and price. To clarify, I'm not Korean and neither have I been to Korea, so I can't say if it's authentic. But to me, it's much better than the one I had in China. Just have a try!</li> </ul> </li> </ol>"},{"location":"SG/Food%20SG/#fine-food-ss","title":"Fine Food (SS)","text":"<ol> <li>Noodle Stall <ul> <li>Price: $5 - $7</li> <li>Recommended: Beef Noodle</li> <li>Review: This is recommended by my fellow teammate in Iot lab Hu Bowen. It's worth mentioning that the noodel is handmade which is quiet precious and rare in Singapore. And the soup is aromatic. As for the beef, I'd give it an average score. However, the biggest problem is the fact that you have to wait in a long queue for so many students and faculty likes to eat here since its a little more convenient. Avoid the peak hours.</li> </ul> </li> </ol>"},{"location":"SG/Food%20SG/#jurong-point","title":"Jurong Point","text":"<ol> <li>Jolibee <ul> <li>Price: $10 - $15</li> <li>Recommended: Chickenjoy</li> <li>Review:  I heard about Jolibee from a bilibili channel called \"meetfood \u89c5\u98df\" and it is believed that Jolibee serves the best fried chicken all over the US. I'm always being willing to give it a shot however it's not accessible in China. However, last week, I happened to find one in Jurong Point and didn't hesitate to try it. I bought three pieces of chickenjoy sided with rice, a small serve of fries as well as a cup of coke. I fisrt tried a fired chicken nugget but it tuned out way too dry and tough to eat. So I have to dip it into the sauce. Speaking of the sauce, it isn\u2019t very thick; it's on the thinner side, but it\u2019s still rich in flavor and has a wonderful or magical aroma. I then tried the drumstick and it was much better than the nugget. Charactirized by the crispy skin and juicy meat yet not outstanding.(But at least better than KFC) So, as a conclusion, this experience fell a little short of my expectations. </li> </ul> </li> <li>GOKOKU Janpanese Bakery <ul> <li>Price: $2 - $4</li> <li>Recommended: </li> <li>Review: </li> </ul> </li> </ol>"},{"location":"SG/TravelPlan/","title":"TravelPlan","text":""},{"location":"SG/TravelPlan/#_1","title":"\u66fc\u8c37\uff1a","text":"<ul> <li>\u4e39\u5ae9\u6c99\u591a\u6c34\u4e0a\u5e02\u573a\uff08\u5750\u8239 \u6930\u5b50\u51b0\u6dc7\u6dcb\uff09</li> <li>\u5510\u4eba\u8857\u591c\u5e02</li> <li>\u5927\u7687\u5bab  \u90d1\u738b\u5e99  \uff08\u7a7f\u957f\u88e4\uff09</li> <li>\u8292\u679c\u7cef\u7c73\u996d\uff0c\u6253\u629b\u996d</li> </ul>"},{"location":"SG/TravelPlan/#_2","title":"\u82ad\u63d0\u96c5\uff1a","text":"<ul> <li>\u683c\u5170\u5c9b\u6c99\u6ee9\uff08\u4e00\u65e5\u6e38\uff0c\u665a\u4e0a\u53ef\u4ee5\u770b\u8868\u6f14\u79c0\uff09</li> <li>\u4eba\u5996\u79c0\uff08\u9644\u8fd1\u80fd\u5403\u4fbf\u5b9c\u6d77\u9c9c\uff09</li> <li>\u6cf0\u62f3\u79c0</li> <li>PinUp Agogo\uff08\u6210\u4eba\u79c0\u597d\u50cf\u8fd8\u884c\uff09</li> <li>\u5b9e\u5f39\u5c04\u51fb</li> </ul>"},{"location":"SG/TravelPlan/#_3","title":"\u82ad\u63d0\u96c5\u7f8e\u98df","text":"<ol> <li>TAAN \u9648\u8bb0</li> <li>kai yok krok</li> <li>lanpo mango sticky rice</li> </ol>"},{"location":"SG/TravelPlan/#bankok","title":"Bankok\u7f8e\u98df","text":"<ol> <li>pad thai kratong thong</li> <li>por pochaya</li> <li>Mango Sticky Rice: korpanit, Mae Varee,palekpayai</li> </ol>"},{"location":"SG/map/","title":"Map","text":"In\u00a0[2]: Copied! <pre>import leafmap.foliumap as leafmap\nfrom ipyleaflet import AwesomeIcon\nicon = AwesomeIcon(\n    name='star',\n    marker_color='green',\n    icon_color='white',\n    spin=False\n)\n</pre> import leafmap.foliumap as leafmap from ipyleaflet import AwesomeIcon icon = AwesomeIcon(     name='star',     marker_color='green',     icon_color='white',     spin=False ) In\u00a0[3]: Copied! <pre>m = leafmap.Map(basemap=\"OpenStreetMap\")\nm.add_basemap('HYBRID')\n# add marker to Singapore\nm.add_marker([1.3521, 103.8198], popup='Singapore')\n# add marker to Johor Bahru\nm.add_marker([1.4655, 103.7578], popup='Johor Bahru')\n# add marker to Bankok and Pattaya\nm.add_marker([13.7563, 100.5018], popup='Bangkok')\nm.add_marker([12.9276, 100.8777], popup='Pattaya')\n# add marker to \u4e09\u4e9a\uff0c\u5e7f\u5dde\uff0c\u6df1\u5733\uff0c\u73e0\u6d77\nm.add_marker([18.2528, 109.5119], popup='\u4e09\u4e9a')\nm.add_marker([23.1291, 113.2644], popup='\u5e7f\u5dde')\nm.add_marker([22.5431, 114.0579], popup='\u6df1\u5733')\nm.add_marker([22.2707, 113.5767], popup='\u73e0\u6d77')\n# add marker to \u4e0a\u6d77\uff0c\u676d\u5dde\uff0c\u82cf\u5dde\uff0c\u5357\u4eac\uff0c\u821f\u5c71\uff0c\u5b81\u6ce2, \u6e56\u5dde, \u6cc9\u5dde\nm.add_marker([31.2304, 121.4737], popup='\u4e0a\u6d77')\nm.add_marker([30.2741, 120.1551], popup='\u676d\u5dde')\nm.add_marker([31.2994, 120.6199], popup='\u82cf\u5dde')\nm.add_marker([32.0603, 118.7969], popup='\u5357\u4eac')\nm.add_marker([30.6394, 122.084], popup='\u821f\u5c71')\nm.add_marker([29.8683, 121.544], popup='\u5b81\u6ce2')\nm.add_marker([30.8943, 120.0868], popup='\u6e56\u5dde')\nm.add_marker([24.8801, 118.6759], popup='\u6cc9\u5dde')\n# add marker to \u91cd\u5e86\uff0c\u6210\u90fd\uff0c\u8d35\u9633\uff0c\u6606\u660e\uff0c\u9075\u4e49\uff0c\u6bd5\u8282\uff0c\u6842\u6797\uff0c\u5317\u6d77\uff0c\u5b9c\u5bbe\uff0c\u4e50\u5c71, \u516d\u76d8\u6c34\uff0c\u5b89\u987a\uff0c\u5357\u5b81\nm.add_marker([29.4316, 106.9123], popup='\u91cd\u5e86')\nm.add_marker([30.5728, 104.0668], popup='\u6210\u90fd')\nm.add_marker([26.5737, 106.5516], popup='\u8d35\u9633')\nm.add_marker([24.8801, 102.8329], popup='\u6606\u660e')\nm.add_marker([27.7257, 106.9272], popup='\u9075\u4e49')\nm.add_marker([27.2985, 105.2519], popup='\u6bd5\u8282')\nm.add_marker([25.2345, 110.1793], popup='\u6842\u6797')\nm.add_marker([21.4832, 109.1194], popup='\u5317\u6d77')\nm.add_marker([28.7519, 104.633], popup='\u5b9c\u5bbe')\nm.add_marker([29.5521, 103.7658], popup='\u4e50\u5c71')\nm.add_marker([26.5874, 104.8319], popup='\u516d\u76d8\u6c34')\nm.add_marker([26.2531, 105.9452], popup='\u5b89\u987a')\nm.add_marker([22.817, 108.3665], popup='\u5357\u5b81')\n# add marker to \u897f\u5b89\uff0c\u5170\u5dde\uff0c\u5f20\u6396\uff0c\u9152\u6cc9\uff0c\u897f\u5b81\uff0c\u5317\u4eac, \u9752\u5c9b\nm.add_marker([34.3416, 108.9398], popup='\u897f\u5b89')\nm.add_marker([36.0611, 103.8343], popup='\u5170\u5dde')\nm.add_marker([38.9256, 100.4498], popup='\u5f20\u6396')\nm.add_marker([39.7329, 98.4945], popup='\u9152\u6cc9')\nm.add_marker([39.9042, 116.4074], popup='\u5317\u4eac')\nm.add_marker([36.0671, 120.3826], popup='\u9752\u5c9b')\nm.add_marker([36.6232, 101.7782], popup='\u897f\u5b81')\n# add marker to \nm\n</pre> m = leafmap.Map(basemap=\"OpenStreetMap\") m.add_basemap('HYBRID') # add marker to Singapore m.add_marker([1.3521, 103.8198], popup='Singapore') # add marker to Johor Bahru m.add_marker([1.4655, 103.7578], popup='Johor Bahru') # add marker to Bankok and Pattaya m.add_marker([13.7563, 100.5018], popup='Bangkok') m.add_marker([12.9276, 100.8777], popup='Pattaya') # add marker to \u4e09\u4e9a\uff0c\u5e7f\u5dde\uff0c\u6df1\u5733\uff0c\u73e0\u6d77 m.add_marker([18.2528, 109.5119], popup='\u4e09\u4e9a') m.add_marker([23.1291, 113.2644], popup='\u5e7f\u5dde') m.add_marker([22.5431, 114.0579], popup='\u6df1\u5733') m.add_marker([22.2707, 113.5767], popup='\u73e0\u6d77') # add marker to \u4e0a\u6d77\uff0c\u676d\u5dde\uff0c\u82cf\u5dde\uff0c\u5357\u4eac\uff0c\u821f\u5c71\uff0c\u5b81\u6ce2, \u6e56\u5dde, \u6cc9\u5dde m.add_marker([31.2304, 121.4737], popup='\u4e0a\u6d77') m.add_marker([30.2741, 120.1551], popup='\u676d\u5dde') m.add_marker([31.2994, 120.6199], popup='\u82cf\u5dde') m.add_marker([32.0603, 118.7969], popup='\u5357\u4eac') m.add_marker([30.6394, 122.084], popup='\u821f\u5c71') m.add_marker([29.8683, 121.544], popup='\u5b81\u6ce2') m.add_marker([30.8943, 120.0868], popup='\u6e56\u5dde') m.add_marker([24.8801, 118.6759], popup='\u6cc9\u5dde') # add marker to \u91cd\u5e86\uff0c\u6210\u90fd\uff0c\u8d35\u9633\uff0c\u6606\u660e\uff0c\u9075\u4e49\uff0c\u6bd5\u8282\uff0c\u6842\u6797\uff0c\u5317\u6d77\uff0c\u5b9c\u5bbe\uff0c\u4e50\u5c71, \u516d\u76d8\u6c34\uff0c\u5b89\u987a\uff0c\u5357\u5b81 m.add_marker([29.4316, 106.9123], popup='\u91cd\u5e86') m.add_marker([30.5728, 104.0668], popup='\u6210\u90fd') m.add_marker([26.5737, 106.5516], popup='\u8d35\u9633') m.add_marker([24.8801, 102.8329], popup='\u6606\u660e') m.add_marker([27.7257, 106.9272], popup='\u9075\u4e49') m.add_marker([27.2985, 105.2519], popup='\u6bd5\u8282') m.add_marker([25.2345, 110.1793], popup='\u6842\u6797') m.add_marker([21.4832, 109.1194], popup='\u5317\u6d77') m.add_marker([28.7519, 104.633], popup='\u5b9c\u5bbe') m.add_marker([29.5521, 103.7658], popup='\u4e50\u5c71') m.add_marker([26.5874, 104.8319], popup='\u516d\u76d8\u6c34') m.add_marker([26.2531, 105.9452], popup='\u5b89\u987a') m.add_marker([22.817, 108.3665], popup='\u5357\u5b81') # add marker to \u897f\u5b89\uff0c\u5170\u5dde\uff0c\u5f20\u6396\uff0c\u9152\u6cc9\uff0c\u897f\u5b81\uff0c\u5317\u4eac, \u9752\u5c9b m.add_marker([34.3416, 108.9398], popup='\u897f\u5b89') m.add_marker([36.0611, 103.8343], popup='\u5170\u5dde') m.add_marker([38.9256, 100.4498], popup='\u5f20\u6396') m.add_marker([39.7329, 98.4945], popup='\u9152\u6cc9') m.add_marker([39.9042, 116.4074], popup='\u5317\u4eac') m.add_marker([36.0671, 120.3826], popup='\u9752\u5c9b') m.add_marker([36.6232, 101.7782], popup='\u897f\u5b81') # add marker to  m   Out[3]:"},{"location":"SG/test/","title":"Test","text":"<p>Enable Click to Add Marker</p>"}]}